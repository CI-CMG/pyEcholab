# coding=utf-8

#    National Oceanic and Atmospheric Administration
#    Alaskan Fisheries Science Center
#    Resource Assessment and Conservation Engineering
#    Midwater Assessment and Conservation Engineering

# THIS SOFTWARE AND ITS DOCUMENTATION ARE CONSIDERED TO BE IN THE PUBLIC DOMAIN
# AND THUS ARE AVAILABLE FOR UNRESTRICTED PUBLIC USE. THEY ARE FURNISHED "AS
# IS." THE AUTHORS, THE UNITED STATES GOVERNMENT, ITS INSTRUMENTALITIES,
# OFFICERS, EMPLOYEES, AND AGENTS MAKE NO WARRANTY, EXPRESS OR IMPLIED, AS TO
# THE USEFULNESS OF THE SOFTWARE AND DOCUMENTATION FOR ANY PURPOSE.  THEY
# ASSUME NO RESPONSIBILITY (1) FOR THE USE OF THE SOFTWARE AND DOCUMENTATION;
# OR (2) TO PROVIDE TECHNICAL SUPPORT TO USERS.

'''
.. module:: echolab2.instruments.EK80

    :synopsis:  A high-level interface for reading SIMRAD ".raw" formatted
                files written by the Simrad EK80 WBES system


| Developed by:  Rick Towler   <rick.towler@noaa.gov>
| National Oceanic and Atmospheric Administration (NOAA)
| Alaska Fisheries Science Center (AFSC)
| Midwater Assesment and Conservation Engineering Group (MACE)
|
| Authors:
|       Rick Towler   <rick.towler@noaa.gov>
| Maintained by:
|       Rick Towler   <rick.towler@noaa.gov>

$Id$
'''

import os
import numpy as np
from scipy.interpolate import interp1d
from .util.simrad_calibration import calibration
from .util.simrad_raw_file import RawSimradFile, SimradEOF
from .util.nmea_data import nmea_data
from .util.motion_data import motion_data
from .util.annotation_data import annotation_data
from .util import simrad_signal_proc
from .util import date_conversion
from .util import simrad_parsers
from ..ping_data import ping_data
from ..processing.processed_data import processed_data
from ..processing import line


class EK80(object):
    """This class is the 'file reader' class for Simrad EK80 instrument files.

    The EK80 class can read in one or more EK80 files and generates a raw_data
    class instance for each unique channel ID in the files. The class also
    contains numerous methods used to extract the raw sample data from one
    channel, or create processed_data objects containing.

    Attributes:
        start_time: Start_time is a datetime object that defines the start
            time of the data within the EK80 class
        end_time: End_time is a datetime object that defines the end time of
            the data within the EK80 class
        start_ping: Start_ping is an integer that defines the first ping of the
            data within the EK80 class.
        end_ping: End_ping is an integer that defines the last ping of
            the data within the EK80 class.
        n_pings: Integer value representing the total number of pings.
        frequencies: List of frequencies read from raw data files.
        channel_ids: List of stings identifying the unique channel IDs read
            from the instrument files.
        n_channels: Integer value representing the total number of
            channels in the class instance.
        raw_data: A dictionary that stores the raw_data objects, one for each
            unique channel in the files. The dictionary keys are the channel
            numbers.
        nmea_data: reference to a NmeaData class instance that will contain
            the NMEA data from the data files.
        read_incremental: Boolean value controlling whether files are read
            incrementally or all at once. The default value is False.
        store_angles: Boolean control variable to set whether or not to store
            angle data generated by EK60 GPTs or WBTs operated in CW
            reduced mode.
        store_power: Boolean control variable to set whether or not to store
            the power data generated by EK60 GPTs or WBTs operated in CW
            reduced mode.
        store_complex: Boolean control variable to set whether or not to store
            the complex data generated by EK80 WBTs.
        read_max_sample_count: Integer value to specify the max sample count
            to read. This property can be used to limit the number of samples
            read (and memory used) when your data of interest is less than
            the total number of samples contained in the EK80 files.
        read_start_time: Datetime64 object containing timestamp of the first
            ping to read if not reading all pings based on time.
        read_end_time: Datetime64 object containing timestamp of last ping to
            read if not reading all ping based on time.
        read_start_ping: Integer ping number of first ping to read if not
            reading all pings based on ping number.
        read_end_ping: Integer ping number of last ping to read if not
            reading all pings based on ping number.
        read_start_sample: Integer sample number of first sample to read if not
            reading all samples based on sample number.
        read_end_sample: Integer sample number of last sample to read if not
            reading all samples based on sample number.
        read_frequencies: List of floats (i.e. 18000.0) specifying the
            frequencies to read. An empty list will result in all frequencies
            being read.
        read_channel_ids = List of strings specifying the channel_ids
            (i,e, 'GPT  38 kHz 009072033fa2 1-1 ES38B') of the channels to
            read. An empty list will result in all channels being read.
    """

    # Specify the default raw data allocation size (in pings) that is used
    # when .idx files are not available.
    DEFAULT_CHUNK_SIZE = 1000

    def __init__(self):
        """Initializes EK80 class object.

        Creates and sets several internal properties used to store information
        about data and control operation of file reading with EK80 object
        instance. Code is heavily commented to facilitate use.
        """

        # Define EK80 properties.

        # These properties control what data are stored when reading
        # EK80 raw files. These properties can be manipulated directly or by
        # specifying them when calling the read and append methods.

        # Set store_angles to true to store angle data if any of the data files
        # contain data recorded from EK60 GPTs or WBT data saved in the reduced
        # power/angle format.
        self.store_angles = True

        # Set store_power to true to store power data if any of the data files
        # contain data recorded from EK60 GPTs or WBT data saved in the reduced
        # power/angle format.
        self.store_power = True

        # Set store_complex to true to store complex data if any of the data files
        # contain data recorded from EK80 WBTs saved as complex samples.
        self.store_complex = True

        # Specify the maximum sample count to read.  This property can be used
        # to limit the number of samples read (and memory used). Samples beyond
        # the maximum will be dropped.
        self.read_max_sample_count = None

        # These params store any limits set on storing of data contained in
        # the raw files read. These parameters can be set directly, or through
        # many of the methods where you can specify start/end time or ping
        # and sample number.
        self.read_start_time = None
        self.read_end_time = None
        self.read_start_ping = None
        self.read_end_ping = None
        self.read_start_sample = None
        self.read_end_sample = None

        # read_frequencies can be set to a list of floats specifying the
        # frequencies to read. An empty list will result in all frequencies
        # being read.
        self.read_frequencies = []

        # read_channel_ids can be set to a list of strings specifying the
        # channel_ids of the channels to read. An empty list will result in all
        # channels being read.
        self.read_channel_ids = []

        # raw_data_width defines the allocation chunk size (in pings) used by
        # the raw data objects. If .idx files are availble, this will be modified
        # on a per file basis based on the number of pings in the .idx file.
        self.raw_data_width = EK80.DEFAULT_CHUNK_SIZE

        #  initialize the data arrays
        self._init()


    def _init(self):
        '''_init is an internal method that initializes the data fields of
        the EK80 class.
        '''

        # The start_time and end_time will define the time span of the
        # data within the EK80 class.
        self.start_time = None
        self.end_time = None

        # The start_ping and end_ping will define the ping span of the data
        # within the EK80 class.
        self.start_ping = None
        self.end_ping = None

        # n_pings stores the total number of pings read.
        self.n_pings = 0

        # n_files stores the total number of raw files read.
        self.n_files = 0

        # A list of stings identifying the channel IDs that have been read.
        self.channel_ids = []

        # n_channels stores the total number of channels in the object.
        self.n_channels = 0

        # A dictionary to store the raw_data objects. The dictionary is keyed
        # by channel ID and each value is a list of raw_data objects associated
        # with that channel.
        self.raw_data = {}

        # A dictionary to store raw_data objects containing the complex Tx samples
        # for channels that were recorded in "extra reduced" power and/or angle data
        # only.
        self.raw_tx = {}

        # channel_number_map maps channel number to channel ID.
        self.channel_number_map = {}

        # frequency_map maps frequency in Hz to channel ID.
        self.frequency_map = {}

        #  nmea_data stores the util.nmea_data object which will contain
        #  the NMEA data read from the raw file. This object has methods
        #  to extract, parse, and interpolate the NMEA data.
        self.nmea_data = nmea_data()

        #  motion_data is the util.motion_data object which will
        #  contain data from the MRU datagrams contained in the raw file.
        #  This object has methods to extract and interpolate the motion data.
        self.motion_data = motion_data()

        # annotations stores the contents of the TAG0 aka "annotation"
        # datagrams.
        self.annotations = annotation_data()

        # data_array_dims contains the dimensions of the sample and angle or
        # complex data arrays specified as [n_pings, n_samples].  Values of
        # -1 specifythat the arrays will resize appropriately to contain all
        # pings and all samples read from the data source.  Setting values > 0
        # will create arrays that are fixed to that size.  Any samples beyond
        # the number specified as n_samples will be dropped.  When a ping is
        # added and the data arrays are full, the sample data will be rolled
        # such that the oldest data is dropped and the new ping is added.
        self._data_array_dims = [-1, -1]

        #  initialize the ping time - used to group "pings"
        self._this_ping_time = np.datetime64('1000-02')

        #  initialize some internal attributes
        self._config = None
        self._filters = {}
        self._tx_params = {}
        self._environment = None
        self._file_n_channels = 0
        self._file_channel_number_map = {}


    def read_raw(self, *args, **kwargs):
        """Reads one or more Simrad EK80 .raw files into memory. Overwrites
        existing data (if any). The data are stored in an EK80.raw_data object.


        Args:
            raw_files (list): List containing full paths to data files to be
                read.
            power (bool): Controls whether power data is stored
            angles (bool): Controls whether angle data is stored
            complex (bool): Controls whether complex data is stored
            max_sample_count (int): Specify the max sample count to read
                if your data of interest is less than the total number of
                samples contained in the instrument files.
            start_time (datetime64): Specify a start time if you do not want
                to start reading from the first ping.
            end_time (datetime64): Specify an end time if you do not want to read
                to the last ping.
            start_ping (int): Specify starting ping number if you do not want
                to start reading at first ping.
            end_ping (int): Specify end ping number if you do not want
                to read all pings.
            frequencies (list): List of floats (i.e. 18000.0) if you
                only want to read specific frequencies.
            channel_ids (list): A list of strings that contain the unique
                channel IDs to read. If no list is supplied, all channels are
                read.
            start_sample (int): Specify starting sample number if not
                reading from first sample.
            end_sample (int): Specify ending sample number if not
                reading to last sample.
        """

        #  initialize the data arrays - this discards any previously read data.
        self._init()

        #  now call append_raw, passing on all arguments
        self.append_raw(*args, **kwargs)


    def read_bot(self, bot_files):
        """Reads .bot formatted bottom detection files created by EK80 systems starting
        with version 2.0.0. Previous versions did not generate .bot files.

              *** You must read the corresponding .raw files first. ***

        This method will read .bot files and insert the bottom detection data in
        the appropriate raw_data object based on channel ID and data type. You can
        use the raw_data.get_bottom() method to get a pyEcholab2 line object
        representing the bottom detections. If you work with the bottom detection
        data directly, remember that the depths are computed using the sound speed
        at the time of collection. If you are using a different sound speed for
        processing, you will need to adjust the raw bottom depths accordingly.

        There are a few caveats and pitfalls that can arise when working with
        .bot data files.

        First, EK80 .bot files lack the a configuration header so it is impossible
        to map bottom detections to a channel ID. Bot file detections are assumed to
        be in the same order as the channels in their corresponding .raw files.
        This means that all of the files you read must have the same channel
        configuration. pyEcholab does not enforce this so

        There is not always a 1:1 relationship between pings in a .bot file
        and pings in a .raw file as was the case with EK60. The bottom detection
        for the first or last ping may be in the previous or next .bot file.

        If heave is being recorded, it is possible that the heave value applied
        to the XYZ bottom detection for a particular ping will be different than
        the heave value recorded in the XML environment datagram for that same
        ping. This will result in a different bottom depth value for that ping.
        It's probable that the heave recording rate plays a roll here.

        Args:
            bot_files (list): A list of .bot files to be read.

        """

        # Ensure that the bot_files argument is a list.
        if isinstance(bot_files, str):
            bot_files = [bot_files]

        # Iterate through the list of .bot files to read.
        for filename in bot_files:

            #  normalize the file path and split out the parts
            filename = os.path.normpath(filename)

            # open the raw file
            fid = RawSimradFile(filename, 'r')

            #  create a variable to track when we're done reading.
            finished = False

            #  and read datagrams until we're done
            while not finished:
                #  read a datagram - method returns some basic info
                dg_info = self._read_datagram(fid, False)

                #  update our finished state var - the file will be finished when
                #  the reader hits the end of the file or if end time/ping are
                #  set and we hit that point.
                finished = dg_info['finished']

        #  close the file
        fid.close()


    def append_raw(self, raw_files, power=None, angles=None, complex=None,
                 max_sample_count=None, start_time=None, end_time=None,
                 start_ping=None, end_ping=None, frequencies=None,
                 channel_ids=None, incremental=None, start_sample=None,
                 end_sample=None, progress_callback=None, nmea=True,
                 callback_ref=None):
        """Reads one or more Simrad EK80 .raw files and appends the data to any
        existing data. The data are ordered as read.


        Args:
            raw_files (list): List containing full paths to data files to be
                read.
            power (bool): Controls whether power data is stored
            angles (bool): Controls whether angle data is stored
            complex (bool): Controls whether complex data is stored
            nmea (bool): Set to True to store NMEA data
            max_sample_count (int): Specify the max sample count to read
                if your data of interest is less than the total number of
                samples contained in the instrument files.
            start_time (datetime64): Specify a start time if you do not want
                to start reading from the first ping.
            end_time (datetime64): Specify an end time if you do not want to read
                to the last ping.
            start_ping (int): Specify starting ping number if you do not want
                to start reading at first ping.
            end_ping (int): Specify end ping number if you do not want
                to read all pings.
            frequencies (list): List of floats (i.e. 18000.0) if you
                only want to read specific frequencies.
            channel_ids (list): A list of strings that contain the unique
                channel IDs to read. If no list is supplied, all channels are
                read.
            start_sample (int): Specify starting sample number if not
                reading from first sample.
            end_sample (int): Specify ending sample number if not
                reading to last sample.
            progress_callback (function reference): Pass a reference to a
                function that accepts four arguments:
                    (filename, cumulative_pct, cumulative_bytes, callback_ref)
                This function will be periodicaly called, passing the
                current file name that is being read, the percent read,
                the bytes read and the callback_userref.
            callback_ref (user defined): Set this to a reference you want
                passed to your callback. This reference will be passed as the
                last argument to the callback function. This can be used, for
                example, to pass a reference to a state variable shared with
                your main application thread allowing the application to
                display the read progress.
        """

        def check_list(val):
            if not isinstance(val, list):
                val = [val]
            return val

        # Update the reader state variables.
        if start_time:
            if not isinstance(start_time, np.datetime64):
                raise TypeError("start_time must be an instance of numpy.datetime64")
            self.read_start_time = start_time
        if end_time:
            if not isinstance(end_time, np.datetime64):
                raise TypeError("end_time must be an instance of numpy.datetime64")
            self.read_end_time = end_time
        if start_ping:
            self.read_start_ping = start_ping
        if end_ping:
            self.read_end_ping = end_ping
        if start_sample:
            self.read_start_sample = start_sample
        if end_sample:
            self.read_end_sample = end_sample
        if power:
            self.store_power = power
        if complex:
            self.store_complex = complex
        if angles:
            self.store_angles = angles
        if max_sample_count:
            self.read_max_sample_count = max_sample_count
        if frequencies:
            self.read_frequencies = check_list(frequencies)
        if channel_ids:
            self.read_channel_ids = check_list(channel_ids)

        # Ensure that the raw_files argument is a list.
        raw_files = check_list(raw_files)

        # Iterate through the list of .raw files to read.
        for filename in raw_files:
            #  reset attributes that store the most recent XML configuration datagram
            #  FIL filter datagrams, XML environment datagrams and XML parameter datagrams
            self._config = None
            self._filters = {}
            self._tx_params = {}
            self._environment = None
            self._file_channel_number_map = {}
            self._file_n_channels = 0
            last_progress = -1

            #  normalize the file path and split out the parts
            filename = os.path.normpath(filename)
            file_parts = filename.split(os.path.sep)
            current_filename = file_parts[-1]
            current_filepath = os.path.sep.join(file_parts[0:-1])

            #  get the total file size
            try:
                total_bytes = os.stat(filename).st_size
            except:
                raise IOError('Unable to open raw file ' + filename + ' for reading.')

            # Check if there is an .idx file for this raw file
            try:
                # assume the .idx file is colocated with the .raw file
                idx_filename = filename[:-3] + 'idx'
                # read the .idx file
                idx_data = self.read_idx(idx_filename)
                # determine the number of pings
                file_n_pings = len(idx_data)
                # make sure the index file is valid
                if file_n_pings == 0:
                    # The index file is invalid - EK80 can generate
                    # empty index files due to a bug?
                    self.raw_data_width = EK80.DEFAULT_CHUNK_SIZE
                else:
                    # set the raw data allocation size to the number of pings+1
                    self.raw_data_width = file_n_pings + 1
            except:
                # Default to an allocation size of 500 pings
                self.raw_data_width = EK80.DEFAULT_CHUNK_SIZE

            # open the raw file
            fid = RawSimradFile(filename, 'r')

            #  read the configuration datagram - this will always be the
            #  first datagram in an EK80 raw file.
            _config = self._read_config(fid)

            #  extract the per channel data into our internal config attribute
            #  and add some additional fields to each channel
            self._config = _config['configuration']
            for channel in self._config:
                self._config[channel]['file_name'] = current_filename
                self._config[channel]['file_path'] = current_filepath
                self._config[channel]['file_bytes'] = total_bytes
                self._config[channel]['start_time'] = _config['timestamp']
                self._config[channel]['start_ping'] = self.n_pings

            #  set the cumulative_bytes var and start time
            cumulative_bytes = _config['bytes_read']

            #  create a variable to track when we're done reading.
            finished = False

            #  and read datagrams until we're done
            while not finished:
                #  read a datagram - method returns some basic info
                dg_info = self._read_datagram(fid, nmea=nmea)

                #  call progress callback if supplied
                if (progress_callback):
                    #  determine the progress as an integer percent.
                    cumulative_bytes += dg_info['bytes_read']
                    cumulative_pct = int(round(float(cumulative_bytes) / total_bytes * 100.))

                    #  call the callback when the percent changes
                    if cumulative_pct != last_progress:
                        #  call the provided callback - the callback has 3 args,
                        #  the first is the full path to the current file and the
                        #  second is the percent read and the third is the bytes read
                        #  and the last is a generic reference that can be set bu the
                        #  caller.
                        progress_callback(filename, cumulative_pct, cumulative_bytes,
                                callback_ref)
                        last_progress = cumulative_pct

                #  update our finished state var - the file will be finished when
                #  the reader hits the end of the file or if end time/ping are
                #  set and we hit that point.
                finished = dg_info['finished']

        #  close the file
        fid.close()

        # Trim excess data from arrays after reading.
        for channel_id in self.channel_ids:
            for raw in self.raw_data[channel_id]:
                raw.trim()
            if channel_id in self.raw_tx:
                self.raw_tx[channel_id].trim()
        self.nmea_data.trim()
        self.motion_data.trim()
        self.annotations.trim()


    def read_idx(self, idx_filename):
        '''read_idx reads a Simrad EK80 .idx file.

        Index files contain the ping number(starting at 1 from when the EK80 application was
        started), the ping time, latitude, longitude, VLW distance, and file offset to the
        first datagram of the ping ensemble for the referenced ping. There is one index datagram
        for each ping and these should exist even if data from a ping was not recorded in the
        raw file.

        '''

        idx_data = {}

        try:
            fid = RawSimradFile(idx_filename, 'r')
        except:
            raise IOError('Unable to open index file ' + idx_filename + ' for reading.')

        # IDX files have a mostly empty XML config header
        _ = fid.read(1)

        try:
            while True:
                new_datagram = fid.read(1)

                # Convert the timestamp to a datetime64 object.
                # Check for NULL datagram date/time which is returned as datetime.datetime(1601, 1, 1, 0, 0)
                if new_datagram['timestamp'].year < 1900:
                    # This datagram has NULL date/time values
                    new_datagram['timestamp'] = np.datetime64("NaT")
                else:
                    # We have a plausible date/time value
                    new_datagram['timestamp'] = \
                            np.datetime64(new_datagram['timestamp'], '[ms]')

                idx_data[new_datagram['ping_number']] = {'distance':new_datagram['distance'],
                        'latitude':new_datagram['latitude'], 'longitude':new_datagram['latitude'],
                        'timestamp':new_datagram['timestamp'], 'file_offset':new_datagram['file_offset']}
        except SimradEOF:
            #  we're at the end of the file
            pass

        return idx_data


    def _read_config(self, fid):
        '''
        _read_config reads the raw file configuration datagram. It then checks if
        if there are any new channels it should be storing and if so, creates
        raw_data objects for those channels and updates some attributes.
        '''

        #  read the configuration datagram - this is the first datagram in an EK80 file.
        config_datagram = fid.read(1)
        config_datagram['timestamp'] = \
                np.datetime64(config_datagram['timestamp'], '[ms]')

        # check for any new channels and add them if required - remove
        # any we aren't reading.
        file_channels = config_datagram['configuration'].keys()
        remove_channels = []
        for channel_id in file_channels:

            # increment the file channel counter and add this channel to the
            # file's channel number map.
            self._file_n_channels += 1
            self._file_channel_number_map[self._file_n_channels] = channel_id

            #  check if we're reading this channel
            if (not self.read_channel_ids or channel_id in self.read_channel_ids):
                #  check if we're reading this frequency
                frequency = config_datagram['configuration'][channel_id]['transducer_frequency']
                if (self.read_frequencies and frequency not in self.read_frequencies):
                    # There are specific frequencies specified and this
                    # is NOT one of them. Mark the channel for removal from
                    # config_datagram dictionary and continue. We can't remove
                    # it inside this loop.
                    remove_channels.append(channel_id)
                    continue

                #  we're reading this channel - check if it's new to us
                if channel_id not in self.raw_data:
                    # This is a new channel, create a list to store this channel's raw_data objects
                    self.raw_data[channel_id] = []

                    #  add the channel to our list of channel IDs
                    self.channel_ids.append(channel_id)

                    #  and increment the channel counter
                    self.n_channels += 1

                    # populate the channel number map
                    self.channel_number_map[self.n_channels] = channel_id

                    # and populate the frequency map
                    if frequency in self.frequency_map:
                        self.frequency_map[frequency].append(channel_id)
                    else:
                        self.frequency_map[frequency] = [channel_id]

                #  check if we need to create an entry for this channel's filters
                if channel_id not in self._filters:
                    #  and an empty dict to store this channel's filters
                    self._filters[channel_id] = {}

        # Now remove any channels marked for removal
        for channel_id in remove_channels:
            config_datagram['configuration'].pop(channel_id)

        # Return the configuration datagram dict
        return config_datagram


    def _read_datagram(self, fid, nmea=True):
        """Reads the next raw file datagram

        This method reads the next datagram from the file, storing the
        data contained in the datagram (if applicable), and returns a dict
        containing the number of bytes read, the datagramn timestamp, datagram
        type, and whether the reader is "finished". The finished flag will be set
        if end_time and/or end_ping is set in the reader and the datagram time
        is after the end time or the internal "ping" counter matches the
        specified end ping.

        Args:
            fid (file object): Pointer to currently open file object. This is a
                RawSimradFile file object and not the standard Python file
                object.

            nmea (bool): Set to True to read NMEA datagrams. If False, NMEA
                datagrams will be discarded. This is primarily used with ES60
                and Mk1 EK60 .bot files that contain a copy of NMEA which is
                also recorded in the .raw file.
        """
        #  create the return dict that provides feedback on progress
        result = {'bytes_read':0, 'timestamp':None, 'type':None, 'finished':False}

        #  attempt to read the next datagram
        try:
            new_datagram = fid.read(1)
        except SimradEOF:
            #  we're at the end of the file
            result['finished'] = True
            return result

        # Convert the timestamp to a datetime64 object.
        # Check for NULL datagram date/time which is returned as datetime.datetime(1601, 1, 1, 0, 0)
        if new_datagram['timestamp'].year < 1900:
            # This datagram has NULL date/time values
            new_datagram['timestamp'] = np.datetime64("NaT")
        else:
            # We have a plausible date/time value
            new_datagram['timestamp'] = \
                    np.datetime64(new_datagram['timestamp'], '[ms]')

        #  update the return dict properties
        result['timestamp'] = new_datagram['timestamp']
        result['bytes_read'] = new_datagram['bytes_read']
        result['type'] = new_datagram['type']

        # If this is a NMEA datagram and we're not storing them, bail
        if not nmea and new_datagram['type'].startswith('NME'):
            # This is a NMEA datagram and we're skipping them
            return result

        # We have to process all XML parameter and environment datagrams
        # regardless of time/ping bounds. This ensures all pings have fresh
        # references to these data.
        if new_datagram['type'].startswith('XML'):
            if new_datagram['subtype'] == 'parameter':
                #  update the most recent parameter attribute for this channel
                self._tx_params[new_datagram[new_datagram['subtype']]['channel_id']] = \
                        new_datagram[new_datagram['subtype']]
            elif new_datagram['subtype'] == 'environment':
                #  update the most recent environment attribute
                self._environment = new_datagram[new_datagram['subtype']]

            return result

        # Check if data should be stored based on time bounds.
        if self.read_start_time is not None:
            if new_datagram['timestamp'] < self.read_start_time:
                #  we have a start time but this data comes before it
                #  so we return without doing anything else
                return result
        if self.read_end_time is not None:
            if new_datagram['timestamp'] > self.read_end_time:
                #  we have a end time and this data comes after it
                #  so we are actually done reading - set the finished
                #  field in our return dict and return
                result['finished'] = True
                return result

        #  update the ping counter
        if new_datagram['type'].startswith('RAW'):
            if self._this_ping_time != new_datagram['timestamp']:
                self.n_pings += 1
                self._this_ping_time = new_datagram['timestamp']

                # check if we're storing this channel
                if new_datagram['channel_id'] not in self.channel_ids:
                    #  no, it's not in the list - just return
                    return result

        # Check if we should store this data based on ping bounds.
        if self.read_start_ping is not None:
            if self.n_pings < self.read_start_ping:
                #  we have a start ping but this data comes before it
                #  so we return without doing anything else
                return result
        if self.read_end_ping is not None:
            if self.n_pings > self.read_end_ping:
                #  we have a end ping and this data comes after it
                #  so we are actually done reading - set the finished
                #  field in our return dict and return
                result['finished'] = True
                return result

        # Update the end_time property.
        if self.end_time is not None:
            # We can't assume data will be read in time order.
            if self.end_time < new_datagram['timestamp']:
                self.end_time = new_datagram['timestamp']
        else:
            self.end_time = new_datagram['timestamp']

        # Process and store the datagrams by type.

        #  FIL datagrams store parameters used to filter the received signal
        #  EK80 class stores the filters for the currently being read file.
        #  Filters are stored by channel ID and then by filter stage
        if new_datagram['type'].startswith('FIL'):

            # Check if we're storing this channel
            if new_datagram['channel_id'] in self.channel_ids:

                #  add the filter parameters for this filter stage
                self._filters[new_datagram['channel_id']][new_datagram['stage']] = \
                        {'stage':new_datagram['stage'],
                         'n_coefficients':new_datagram['n_coefficients'],
                         'decimation_factor':new_datagram['decimation_factor'],
                         'coefficients':new_datagram['coefficients']}

        # RAW datagrams store raw acoustic data for a channel.
        elif new_datagram['type'].startswith('RAW'):

            # Check if we're storing this channel
            if new_datagram['channel_id'] in self.channel_ids:

                #  check if we should set our start time property
                if not self.start_time:
                    self.start_time = new_datagram['timestamp']
                # Set the first ping number we read.
                if not self.start_ping:
                    self.start_ping = self.n_pings
                # Update the last ping number.
                self.end_ping = self.n_pings

                # Add the raw datagram based on the RAW datagram type.
                if new_datagram['type'][3] == '3':

                    # RAW3 datagrams store the "regular" sample data

                    #  loop through the raw_data objects to find the raw_data object
                    #  to store this data.
                    this_raw_data = None
                    for raw_obj in self.raw_data[new_datagram['channel_id']]:
                        if raw_obj.data_type == '':
                            #  This raw_data object has no type so is empty and can store anything
                            this_raw_data = raw_obj
                            break
                        elif (new_datagram['data_type'] == 1 and raw_obj.data_type == 'power' or
                              new_datagram['data_type'] == 2 and raw_obj.data_type == 'angle' or
                              new_datagram['data_type'] == 3 and raw_obj.data_type == 'power/angle'):
                            # This raw_data object contains the dame data type as the datagram
                            this_raw_data = raw_obj
                            break
                        elif new_datagram['data_type'] > 3:
                            #  this will be a complex type
                            #  first check if the number of sectors are the same
                            if new_datagram['n_complex'] == raw_obj.complex.shape[2]:
                                # Then make sure the data types are the same
                                is_fm = self._tx_params[new_datagram['channel_id']]['pulse_form'] > 0

                                if is_fm and raw_obj.data_type == 'complex-FM':
                                    this_raw_data = raw_obj
                                elif not is_fm and raw_obj.data_type == 'complex-CW':
                                    this_raw_data = raw_obj
                                break

                    #  check if we need to create a new raw_data object
                    if this_raw_data is None:
                        #  create the new raw_data object
                        this_raw_data = raw_data(new_datagram['channel_id'],
                                store_power=self.store_power,
                                store_angles=self.store_angles,
                                store_complex=self.store_complex,
                                max_sample_number=self.read_max_sample_count,
                                n_pings=self.raw_data_width)
                        # Set the transceiver type
                        this_raw_data.transceiver_type = \
                                self._config[new_datagram['channel_id']]['transceiver_type']
                        # Set the motion_data and nmea_data references
                        this_raw_data.motion_data = self.motion_data
                        this_raw_data.nmea_data = self.nmea_data

                        #  and add it to this channel's list of raw_data objects
                        self.raw_data[new_datagram['channel_id']].append(this_raw_data)

                    # Call this channel's append_ping method.
                    this_raw_data.append_ping(new_datagram,
                            self._config[new_datagram['channel_id']], self._environment,
                            self._tx_params[new_datagram['channel_id']],
                            self._filters[new_datagram['channel_id']],
                            start_sample=self.read_start_sample,
                            end_sample=self.read_end_sample)

                elif new_datagram['type'][3] == '4':
                    # RAW4 datagrams are only generated when saving reduced sample data
                    # (power and/or angle data) and they contain the first 32 complex
                    # samples from the corresponding RAW3 datagram for each channel.

                    if not new_datagram['channel_id'] in self.raw_tx:

                        #  create the new raw_data object to hold the Tx data
                        this_raw_tx = raw_data(new_datagram['channel_id'],
                                store_power=False,
                                store_angles=False,
                                store_complex=True,
                                max_sample_number=self.read_max_sample_count,
                                n_pings=self.raw_data_width)
                        # Set the transceiver type
                        this_raw_tx.transceiver_type = \
                                self._config[new_datagram['channel_id']]['transceiver_type']
                        # Set the motion_data and nmea_data references
                        this_raw_tx.motion_data = self.motion_data
                        this_raw_tx.nmea_data = self.nmea_data

                        #  and this channel to the raw_tx dict
                        self.raw_tx[new_datagram['channel_id']] = this_raw_tx

                    # get a reference to this channels
                    this_raw_tx = self.raw_tx[new_datagram['channel_id']]

                    # Call this channel's append_ping method.
                    this_raw_tx.append_ping(new_datagram,
                            self._config[new_datagram['channel_id']], self._environment,
                            self._tx_params[new_datagram['channel_id']],
                            self._filters[new_datagram['channel_id']],
                            start_sample=self.read_start_sample,
                            end_sample=self.read_end_sample)

        # NME datagrams store ancillary data as NMEA-0183 style ASCII data.
        elif new_datagram['type'].startswith('NME'):
            # Add the datagram to our nmea_data object.
            self.nmea_data.add_datagram(new_datagram['timestamp'],
                    new_datagram['nmea_string'])

        # TAG datagrams contain time-stamped annotations inserted via the
        # recording software. They are not associated with a specific channel
        elif new_datagram['type'].startswith('TAG'):
            # Add this datagram to our annotation_data object
            self.annotations.add_datagram(new_datagram['timestamp'],
                    new_datagram['text'])

        # MRU datagrams contain vessel motion data
        elif new_datagram['type'].startswith('MRU'):
            # append this motion datagram to the motion_data object
            self.motion_data.add_datagram(new_datagram['timestamp'],
                    new_datagram['heave'], new_datagram['pitch'],
                    new_datagram['roll'], new_datagram['heading'])

        # BOT datagrams contain bottom detections
        elif new_datagram['type'].startswith('BOT'):
            # iterate through the channels in this .bot file
            for idx, depth in enumerate(new_datagram['depth']):
                # Get this detection's channel ID
                # Note that this only works if the channels in the .raw file(s)
                # read match the channels in the .bot file. EK80 bot files provide
                # no channel ID information so a more robust mapping is not possible.
                this_chan = self._file_channel_number_map.get(idx+1, None)

                # Check if we have data for this channel
                if this_chan in self.raw_data:
                    # This is hokey, but there is no way to know which data object
                    # this depth applies to so we just try for all data objects.
                    for data in self.raw_data[this_chan]:
                        # In order to avoid checking the index array below for every
                        # chan/datatype, we'll check for and add if needed the
                        # bottom detection attribute to alldata objects if any
                        # bottom data is read.
                        if not hasattr(data, 'detected_bottom'):
                            # This data object doesn't have the detected_bottom attribute.
                            # Create and add it.
                            new_attr = np.full((data.ping_time.shape), np.nan, np.float32)
                            data.add_data_attribute('detected_bottom', new_attr)

                        # Get the index of this detection and insert into data object
                        ping_idx = data.ping_time == new_datagram['timestamp']
                        data.detected_bottom[ping_idx] = new_datagram['depth'][idx]

        else:
            #  report an unknown datagram type
            print("Unknown datagram type: " + str(new_datagram['type']))

        #  return our result dict which contains some basic info about
        #  the datagram we just read
        return result


    def get_channel_data(self, frequencies=None, channel_ids=None,  channel_numbers=None):
        """returns a dict containing lists of raw_data objects for the specified channel IDs,
        or frequencies, or channel numbers.

        This method returns a dictionary of lists that contain references to one or
        more raw_data objects containing the data that has been read. The dictionary keys
        and the raw_data object(s) returned depend on the argument(s) passed. You can
        specify one or more frequencies, channel numbers, and/or channel IDs and the
        dictionary will be keyed by frequency, channel number, and/or channel ID and
        the key values will be a list containing the raw_data objects that correspond
        to that frequency, channel number, or channel ID.

        If you specify one or more frequencies, this method will return a dict, keyed
        by frequency where the values are a lists containing one or more raw_data objects
        associated with the specified frequency. The length of the list is equal to the
        number of unique channel + datatype combinations sharing that transmit frequency.

        If you specify one or more channel IDs, this method will return a dict, keyed
        by channel ID where the values are a lists containing one or more raw_data objects
        associated with the specified channel ID. The length of the list is equal to the
        number of unique channel + datatype combinations sharing that channel ID.

        And, as you've already guessed, if you specify one or more channel numbers, this
        method will return a dict, keyed by channel number where the values are a lists
        containing one or more raw_data objects associated with the specified channel
        number. The length of the list is equal to the number of unique channel + datatype
        combinations sharing that channel number.

        If called without arguments, it returns a dictionary, keyed by channel ID, that
        contains all channels read. The values are a lists containing one or more raw_data
        objects associated with the specified channel ID just as if you called the method
        specifying all channel IDs.

        You can specify multiple keyword arguments to return references to the data mapped
        in multiple ways if your analysis requires that flexibility. This method only returns
        references, it does not copy the data, so there is no real penalty for doing this.
        Do remember that if there is a valid reference to an object in memory it will continue
        to exist and consume RAM. If memory management is important for your application
        you will want to manage these and not just grab a bunch of references willy-nilly.

        Args:
            frequencies (float, list): Specify one or more frequencies in a list to return
                    data objects associated with that frequency.
            channel_ids (str, list): Specify one or more channel IDs in a list to return
                    data objects associated with that channel ID.
            channel_numbers (int, list): Specify one or more channel numbers in a list to
                    return data objects associated with that channel number.
        Returns:
            Dictionary keyed by frequency/channel_id/channel_number. Values are lists
            of raw_data objects associated with the specified frequency/channel_id/
            channel_number.
        """

        # create the return dict
        channel_data = {}

        if channel_ids is not None:
            # Channel id is specified.

            # if we're passed a string, make it a list
            if isinstance(channel_ids, str):
                channel_ids = [channel_ids]

            #  work thru the channel ids and add if they exist
            for id in channel_ids:
                channel_data[id] = self.raw_data.get(id, None)

        elif frequencies is not None:
            # frequency is specified

            # if we're passed a number, make it a list
            if not isinstance(frequencies, list):
                frequencies = [frequencies]

            # and work through the freqs, adding if they exist
            for freq in frequencies:
                # There is not a 1:1 mapping of freqs to data so we have to
                # create a list and then extend it for each matching frequency.
                channel_data[freq] = []

                data_objs = self.frequency_map.get(freq, [None])
                for id in data_objs:
                    channel_data[freq].extend(self.raw_data[id])

        elif channel_numbers is not None:
            # channel_number is specified.

            # if we're passed a number, make it a list
            if not isinstance(channel_numbers, list):
                channel_numbers = [channel_numbers]

            # iterate thru the list of channel numbers, extracting the data
            for num in channel_numbers:
                try:
                    id = self.channel_number_map[num]
                    channel_data[num] = self.raw_data.get(id, None)
                except:
                    pass

        else:
            # No keywords specified - return all in a dict keyed by channel ID
            channel_data = self.raw_data

        return channel_data


    def write_raw(self, output_filenames, power=True, angles=True,
                 complex=True, max_sample_count=None, start_time=None,
                 end_time=None, start_ping=None, end_ping=None, frequencies=None,
                 channel_ids=None, start_sample=None, end_sample=None, progress_callback=None,
                 overwrite=False, async_window=5, reduce_complex_precision=False,
                 raw_index_array=None, nmea_index_array=None, callback_ref=None,
                 annotation_index_array=None, strip_padding=True):
        """Writes one or more Simrad EK80 .raw files.


        Args:
            output_filenames (str, dict): A string specifying the full path and
                output filename header that will be used to generate all of the
                output filenames OR a dict, keyed by *input* filename where the
                values specify the full path and *full* output filename that will
                be used when writing data associated with that input file.

                Providing a string will result in behavior similar to ER60 software
                where you provide the folder and filename header and it generates
                data in that folder naming the files with the header and appending
                the date and time:

                    output_filenames = 'C:\temp\test_ek80_file'

                would result in files named similar to:

                    C:\temp\test_ek80_file-D202020101-T120000.raw
                    C:\temp\test_ek80_file-D202020101-T122513.raw

                If, in the off chance that you have read data from two files that
                both start at the same time, the subsequent files will have a
                "-#" appended to the time, to ensure that the file names are
                unique.

                Providing a dict, keyed by input filename will allow full control
                of the output file naming. It is your responsibility to provide a
                dict where the keys map 1:1 to the input file names. If they do
                not, an error will be raised.

            power (bool): Controls whether power data is written. Has no effect if
                there is no power data to write.

            angles (bool): Controls whether angle data is written. Has no effect if
                there is no angle data to write.

            complex (bool): Controls whether complex data is written. Has no effect if
                there is no complex data to write.

            start_time (datetime64): Specify a start time when defining a range
                of data to write. When start_time is not provided, the start
                time will be the time of the first ping.

            end_time (datetime64): Specify a end time when defining a range
                of data to write. When end_time is not provided, the end
                time will be the time of the last ping.

            start_ping (int): Specify the starting ping number when defining a
                range of pings to write. If start_ping is not specified,
                the start ping is 1.

            end_ping (int): Specify the end ping number when defining a range
                of pings to write. If end_ping is not provided, the last
                ping is used.

            frequencies (list): List of floats specifying the frequencies in
                Hz you want to write. If frequencies is not specified, all
                frequencies will be written.

            channel_ids (list): A list of strings that contain the unique
                channel IDs to write. If no list is supplied, all channels are
                written.

            start_sample (int): Specify starting sample number if not
                writing from first sample.

            end_sample (int): Specify ending sample number if not
                writing to last sample.

            reduce_complex_precision (bool): Set to True to write complex data
                as 16-bit values. This has no effect if the data were originally
                recorded as 16-bit values but can reduce the file sizes of files
                where complex data is stored as 32-bit values.

            strip_padding (bool): Set to True to remove any NaN padding at the
                beginning and/or end of the sample data. Echolab stores sample data in
                rectangular arrays. When the sample number increases within or
                between files, these arrays are resized and pings with fewer samples
                are padded with NaNs as appropriate. When True, these samples are
                omitted when writing. This DOES NOT affect padded pings (entire pings
                that contain no data.) Padded pings are always removed before writing
                because they lack not only sample data, but all synchronous ping
                attributes. The default value is True.

            progress_callback (function reference): Pass a reference to a
                function that accepts four arguments:
                    (filename, cumulative_pct, cumulative_bytes, callback_ref)
                This function will be periodicaly called, passing the
                current file name that is being read, the percent read,
                the bytes read and the callback_userref.

            callback_ref (user defined): Set this to a reference you want
                passed to your callback. This reference will be passed as the
                last argument to the callback function. This can be used, for
                example, to pass a reference to a state variable shared with
                your main application thread allowing the application to
                display the read progress.

            The following keywords are for advanced indexing and writing of the
            data. This can allow you to easily write subsets of data where the
            pings do not have to be contiguous.

            raw_index_array (dict): Set this to a dictionary, keyed by
                raw_data object reference, where the values are index arrays that
                specify the pings to write for each raw_data object. If you specify
                this keyword, the start/stop time/ping and time_order keywords will
                be ignored.

            THE FOLLOWING KEYWORDS ARE NOT IMPLEMENTED YET

            nmea_index_array (np.array):Set this to an index array that specifies
                the NMEA data that will be written to the raw file. This will
                override the default behavior of using the data start and end times
                (including when raw_index_array is defined.)

            annotation_index_array (np.array):Set this to an index array that specifies
                the annotation data that will be written to the raw file. This will
                override the default behavior of using the data start and end times
                (including when raw_index_array is defined.)

        """

        def new_datagram(dg_time, dg_type, dg_version):
            '''
            new_datagram builds a dict containing the keys common to all datagram
            dictionaries.
            '''

            # build the base datagram dict. These fields are common to all datagrams
            dgram = {}
            low_date, high_date = date_conversion.dt64_to_nt(dg_time)
            dgram['low_date'] = low_date
            dgram['high_date'] = high_date
            dgram['type'] = dg_type + str(dg_version)
            dgram['dg_version'] = dg_version

            return dgram


        def remove_sample_padding(data, sample_offset):
            '''
            remove_sample_padding will strip NaN padding from the beginning and
            end of a "ping". It adjusts the sample_offset if required. It does
            not alter data between the first and last non NaN values.
            '''

            # check for any nan data
            if data.ndim == 1:
                nan_idx = np.isnan(data)
            else:
                nan_idx = np.isnan(data[:,0])

            # Check if there are any NaNs at all
            if np.any(nan_idx):

                # find the data start and data end indexes
                start_idx = np.argmin(nan_idx)
                end_idx = np.argmax(nan_idx)

                # update the sample_offset if needed
                if start_idx != sample_offset:
                    sample_offset = start_idx

                # get data with padding removed
                if data.ndim == 1:
                    data = data[start_idx:end_idx]
                else:
                    data = data[start_idx:end_idx,:]

            return (data, sample_offset)


        # Initialize a list to hold the filenames of the files we write
        files_written = []

        # Process the args
        if start_time:
            start_time = start_time
        else:
            start_time = self.start_time

        if end_time:
            end_time = end_time
        else:
            end_time = self.end_time

        if channel_ids is None:
            channel_ids = self.channel_ids

        if start_sample is None:
            start_sample = 0

        if start_sample < 0:
            start_sample = 0

        if end_sample:
            if end_sample <= start_sample:
                end_sample = start_sample

        # Create the async data time delta - used when extracting async data
        # like NMEA or TAG for writing
        async_window_secs = np.timedelta64(async_window, 's')

        #  map dg_type to its parser
        DGRAM_PARSE_KEY = {'RAW': simrad_parsers.SimradRawParser(),
                           'FIL': simrad_parsers.SimradFILParser(),
                           'TAG': simrad_parsers.SimradAnnotationParser(),
                           'NME': simrad_parsers.SimradNMEAParser(),
                           'MRU': simrad_parsers.SimradMRUParser(),
                           'XML': simrad_parsers.SimradXMLParser()}

        # map dg_type to version for this class/raw file spec
        DGRAM_VERSION_KEY = {'RAW':3,
                             'NME':0,
                             'FIL':1,
                             'TAG':0,
                             'MRU':0,
                             'XML':0}

        # Because a user can load multiple .raw files into an EK80 object, and those
        # .raw files can contain data from various system configurations, we need to
        # write potentially different configurations as separate files to ensure the
        # new data are interpreted correctly when read.
        #
        # Each ping in a raw_data object references a dict containing the system
        # configuration data for that channel as written in the .raw file header. That
        # dict also contains the input raw file name. The unique raw file names can be
        # used to determine how many files to write and the channel data that will be
        # in them.
        #
        # This is important to remember if you are trying to combine data from two raw
        # files into one. It is your responsibility to only combine data from files
        # recorded using the the same system parameters. After combining data from
        # two or more files, you need to replicate the first configuration dict in each
        # raw_data object across all pings:
        #
        # raw_data.configuration[:] = raw_data.configuration[0]

        # The first thing we need to do is identify all of the data that meets our
        # time/ping number, channel/frequency constraints. Then we group that by input
        # file name and again by channel. The result is a dict, keyed by file name,
        # with values that are dicts keyed by the channel ID

        # Work thru the specified channels
        data_by_file = {}
        n_datagrams_by_file = {}
        for channel in channel_ids:
            # and then this channel's raw_data objects

            for data in self.raw_data[channel]:
                # Check if we've been provided with an index array
                if raw_index_array is not None:
                    if isinstance(raw_index_array, dict) and data in raw_index_array:
                        # this is a dict mapping the index to theraw_data objects
                        this_idx = raw_index_array[data]
                    else:
                        # Assume that if we're not passed a dict, that this must be
                        # an index array that will be applied to all channels
                        this_idx = raw_index_array
                else:
                    # No index array provided. Get the indices of data from this
                    # channel/data that fall within the time span provided.
                    this_idx = data.get_indices(start_time=start_time,
                            end_time=end_time, start_ping=start_ping,
                            end_ping=end_ping)

                # check if we have any data from this channel/data
                if this_idx.size > 0:
                    # Find the unique configuration objects for this data/time range
                    # The config objects are different for each file read and we use
                    # that to determine what data goes in what output file.
                    #
                    # We can't use np.unique on dicts and the dict == operator does
                    # a deep comparison which fails when the values are numpy arrays.
                    #
                    # Instead we have to brute force this using the dict object ID.
                    # We still need a reference to the unique dicts so we have to
                    # keep a list of both the ID and dict. As we're doing the comparison
                    # we'll build an array of object IDs which we use below when
                    # identifying pings with each unique configuration.
                    unique_configs = []
                    unique_ids = []
                    conf_ids = []
                    for c in data.configuration:
                        # make sure this ping has a valid config. Padded pings will
                        # have NaNs so we'll ignore those. They will be removed later.
                        if isinstance(c, dict):
                            this_id = id(c)
                            conf_ids.append(this_id)
                            is_unique = True
                            for uc in unique_configs:
                                if uc['file_name'] == c['file_name']:
                                    is_unique = False
                        else:
                            is_unique = False
                            this_id = None
                            conf_ids.append(this_id)

                        if is_unique:
                            # this is a convienient place to filter by frequency...
                            if frequencies:
                                #  freqs specified, check if this is one of them
                                if c['frequency'] in frequencies:
                                    unique_ids.append(this_id)
                                    unique_configs.append(c)
                            else:
                                # writing all freqs
                                unique_ids.append(this_id)
                                unique_configs.append(c)

                    # Convert the list of cinfiguration IDs to a numpy array
                    conf_ids = np.array(conf_ids)

                    # Determine the indices for each config
                    for idx in range(len(unique_configs)):
                        # Get this config object and object ID
                        conf = unique_configs[idx]
                        conf_id = unique_ids[idx]

                        # Get the indices for each unique config - if we are passed a
                        # boolean array, we have to convert it to an index array.
                        if this_idx.dtype == np.bool_:
                            conf_idx = np.nonzero(np.logical_and(conf_ids == conf_id,
                                    this_idx))[0]
                        else:
                            conf_idx = this_idx[conf_ids[this_idx] == conf_id]

                        # To track progress, we'll total up the number of raw
                        # datagrams that we will ultimately write for this file.
                        n_datagrams = conf_idx.size

                        # Store the channel ID, config dict, reference to the
                        # raw data, and the index to the data by filename
                        this_data = {'channel_id':channel, 'configuration':conf,
                                'data':data, 'index':conf_idx}

                        if conf['file_name'] not in data_by_file:
                            # this is a new file, add it
                            data_by_file[conf['file_name']] = {channel:[this_data]}
                            # and set the initial datagram count
                            n_datagrams_by_file[conf['file_name']] = n_datagrams
                        else:
                            # this file is already on the list, check if the channel exists (this
                            # is the rare, but possible, multiple data types per channel case.)
                            if channel in data_by_file[conf['file_name']]:
                                data_by_file[conf['file_name']][channel].append(this_data)
                            else:
                                data_by_file[conf['file_name']][channel] = [this_data]
                            n_datagrams_by_file[conf['file_name']] += n_datagrams

        # Now that we have references to the data we need to write, grouped by input
        # filename, we can iterate through the this dict and write the data to disk.
        for infile in data_by_file:

            outfile_name = ''

            # check if we've been given a dict of filenames and if we have one for
            # this input file.
            if isinstance(output_filenames, dict):
                if infile in output_filenames:
                    outfile_name = output_filenames[infile]
                else:
                    raise ValueError("Input filename '" + infile + "' not found in " +
                        "output_filenames dictionary. No output filename available to " +
                        "map to.")

            # .raw files are always written in time order. We need to create a vector
            # of timestamps from all of our data sources (raw data, NMEA data, annotations,
            # and motion data) and use the sort indices to create a our datagram roadmap.

            # create the map arrays
            dg_times = np.array([], dtype='datetime64[ms]')
            dg_objects = np.array([], dtype='object')
            dg_obj_idx = np.array([], dtype=np.uint32)
            dg_type = np.array([], dtype='S3')

            # Add the raw data
            for channel in data_by_file[infile]:
                for data in data_by_file[infile][channel]:
                    # First, remove any empty pings from the index.
                    data['index'] = data['index'][np.isfinite(data['data'].channel_mode[data['index']])]
                    # Then extract the times and references to the data we're writing
                    times = data['data'].ping_time[data['index']]
                    dg_times = np.concatenate((dg_times, times))
                    dg_objects = np.concatenate((dg_objects, np.repeat(data['data'], times.size)))
                    dg_obj_idx = np.concatenate((dg_obj_idx, data['index']))
                    dg_type = np.concatenate((dg_type, np.repeat('RAW', times.size)))

            # Determine the data time window
            raw_start_time = dg_times[0]
            async_start_time = dg_times[0] - async_window_secs
            async_end_time = dg_times[-1] - async_window_secs

            # Get the unique environment datagrams - we can use the reference to the
            # last data object in the last channel that exists after adding the raw
            # data above to get the environment data. The references to the environment
            # datagrams are shared between all channels.
            data_by_file[infile][channel]
            _, env_idx = np.unique(np.array([id(xi) for xi in data['data'].environment]),
                    return_index=True)
            times = data['data'].ping_time[env_idx]
            dg_times = np.insert(dg_times, 0, times)
            dg_objects = np.insert(dg_objects, 0, data['data'].environment[env_idx])
            dg_obj_idx = np.insert(dg_obj_idx, 0, env_idx)
            dg_type = np.insert(dg_type, 0, np.repeat('ENV', times.size))

            # Next, add the annotation data
            annotation_idx = self.annotations.get_indices(start_time=async_start_time,
                end_time=async_end_time)
            times = self.annotations.times[annotation_idx]
            dg_times = np.insert(dg_times, 0, times)
            dg_objects = np.insert(dg_objects, 0, np.repeat(self.annotations, times.size))
            dg_obj_idx = np.insert(dg_obj_idx, 0, annotation_idx)
            dg_type = np.insert(dg_type, 0, np.repeat('TAG', times.size))

            # Then the motion data using our new time window
            motion_idx = self.motion_data.get_indices(start_time=async_start_time,
                        end_time=async_end_time)
            times = self.motion_data.times[motion_idx]
            dg_times = np.insert(dg_times, 0, times)
            dg_objects = np.insert(dg_objects, 0, np.repeat(self.motion_data, times.size))
            dg_obj_idx = np.insert(dg_obj_idx, 0, motion_idx)
            dg_type = np.insert(dg_type, 0, np.repeat('MRU', times.size))

            # Lastly, insert the NMEA data using our new time window
            nmea_idx = self.nmea_data.get_indices(start_time=async_start_time,
                        end_time=async_end_time)
            times = self.nmea_data.nmea_times[nmea_idx]
            dg_times = np.insert(dg_times, 0, times)
            dg_objects = np.insert(dg_objects, 0, np.repeat(self.nmea_data, times.size))
            dg_obj_idx = np.insert(dg_obj_idx, 0, nmea_idx)
            dg_type = np.insert(dg_type, 0, np.repeat('NME', times.size))

            # Now sort the time vector - must use stable sort to ensure proper dg order
            sorted_time_idx = np.argsort(dg_times, kind='stable')

            # Sort the map arrays using the sorted time index
            dg_times = dg_times[sorted_time_idx]
            dg_objects = dg_objects[sorted_time_idx]
            dg_obj_idx = dg_obj_idx[sorted_time_idx]
            dg_type = dg_type[sorted_time_idx]
            n_datagrams = dg_times.size

            # we are finally ready to write...

            # Generate output filename if needed - we assume we're passed
            # a valid path and filename prefix. e.g. 'c:/test/DY2020'
            if outfile_name == '':
                timestamp = date_conversion.dt64_to_datetime(raw_start_time)
                timestamp = timestamp.strftime("D%Y%m%d-T%H%M%S")
                outfile_name = output_filenames + '-' + timestamp + '.raw'

            # open the raw file
            outfile_name = os.path.normpath(outfile_name)
            if not overwrite and os.path.exists(outfile_name):
                raise IOError('File %s already exists and overwrite == False.' %(outfile_name))
            raw_fid = open(outfile_name, 'wb')

            # Initialize some vars to track our progress
            bytes_written = 0
            cumulative_pct = -1
            datagrams_processed = 0

            #  keep track of the files we're writing to return to caller
            files_written.append(outfile_name)

            # Build the configuration dict and collect the filter params
            filter_params = {}
            config_dgram = new_datagram(dg_times[0], 'XML', DGRAM_VERSION_KEY['XML'])
            config_dgram['configuration'] = {}
            config_dgram['subtype'] = 'configuration'
            for channel in data_by_file[infile]:
                filter_params[channel] = data_by_file[infile][channel][0]['data'].filters[0]
                config_dgram['configuration'][channel] = \
                    data_by_file[infile][channel][0]['configuration']

            #  pack config dict into raw byte stream and write
            bytes_written += raw_fid.write(DGRAM_PARSE_KEY['XML'].to_string(config_dgram))

            # Now write the filter datagrams
            for channel in filter_params:
                for stage in filter_params[channel]:
                    # Create a FIL datagram
                    filter_dgram = new_datagram(dg_times[0], 'FIL', DGRAM_VERSION_KEY['FIL'])
                    filter_dgram['channel_id'] = channel
                    filter_dgram['stage'] = stage
                    filter_dgram['n_coefficients'] = filter_params[channel][stage]['n_coefficients']
                    filter_dgram['decimation_factor'] = filter_params[channel][stage]['decimation_factor']
                    filter_dgram['coefficients'] = filter_params[channel][stage]['coefficients']

                    # And write
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY['FIL'].to_string(filter_dgram))


            #  now write out all the rest of the datagrams
            for idx in range(n_datagrams):

                #  now assemble the datagram dict based on the type
                if dg_type[idx] == 'RAW':

                    #  get the channel ID
                    channel_id = data_by_file[infile][dg_objects[idx].channel_id][0]['configuration']['channel_id']

                    # Create the base datagram dict for the XML Parameter datagram
                    dgram = new_datagram(dg_times[idx], 'XML', DGRAM_VERSION_KEY['XML'])

                    # Build the datagram dictionary
                    dgram['subtype'] = 'parameter'
                    dgram['parameter'] = {}
                    dgram['parameter']['channel_id'] = channel_id
                    dgram['parameter']['channel_mode'] = dg_objects[idx].channel_mode[dg_obj_idx[idx]]
                    dgram['parameter']['pulse_form'] = dg_objects[idx].pulse_form[dg_obj_idx[idx]]
                    if dg_objects[idx].data_type == 'complex-FM':
                        dgram['parameter']['frequency_start'] = dg_objects[idx].frequency_start[dg_obj_idx[idx]]
                        dgram['parameter']['frequency_end'] = dg_objects[idx].frequency_end[dg_obj_idx[idx]]
                    else:
                        dgram['parameter']['frequency'] = dg_objects[idx].frequency[dg_obj_idx[idx]]
                    dgram['parameter']['pulse_duration'] = dg_objects[idx].pulse_duration[dg_obj_idx[idx]]
                    dgram['parameter']['sample_interval'] = dg_objects[idx].sample_interval[dg_obj_idx[idx]]
                    dgram['parameter']['transmit_power'] = dg_objects[idx].transmit_power[dg_obj_idx[idx]]
                    dgram['parameter']['slope'] = dg_objects[idx].slope[dg_obj_idx[idx]]

                    # write theparameter XML datagram
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY['XML'].to_string(dgram))

                    # Create the base datagram dict for the RAW3 datagram
                    dgram = new_datagram(dg_times[idx], dg_type[idx], DGRAM_VERSION_KEY[dg_type[idx]])

                    # Build the datagram dictionary starting with channel ID
                    dgram['channel_id'] = channel_id

                    # adjust the sample offset if we're creating an additional offset during writing
                    sample_offset = start_sample + dg_objects[idx].sample_offset[dg_obj_idx[idx]]

                    # Extract the data, strip padding, and update the sample count
                    if hasattr(dg_objects[idx], 'power'):
                        power_data = dg_objects[idx].power[dg_obj_idx[idx],start_sample:end_sample]
                        if strip_padding:
                            power_data, sample_offset = remove_sample_padding(power_data, sample_offset)
                        sample_count = power_data.shape[0]
                    if hasattr(dg_objects[idx], 'angles_athwartship_e'):
                        angles_athwart_data = dg_objects[idx].angles_athwartship_e[dg_obj_idx[idx],start_sample:end_sample]
                        angles_along_data = dg_objects[idx].angles_alongship_e[dg_obj_idx[idx],start_sample:end_sample]
                        if strip_padding:
                            angles_athwart_data, sample_offset = remove_sample_padding(angles_athwart_data, sample_offset)
                            angles_along_data, sample_offset = remove_sample_padding(angles_along_data, sample_offset)
                        sample_count = angles_athwart_data.shape[0]
                    if hasattr(dg_objects[idx], 'complex'):
                        complex_data = dg_objects[idx].complex[dg_obj_idx[idx],start_sample:end_sample,:]
                        if strip_padding:
                            complex_data, sample_offset = remove_sample_padding(complex_data, sample_offset)
                        sample_count = complex_data.shape[0]

                    # update the sample count and offset
                    dgram['count'] = sample_count
                    dgram['offset'] = sample_offset

                    #  initialize the datatype
                    datatype = 0

                    # If we have angle data, we have to convert it back to indexed and combine.
                    if hasattr(dg_objects[idx], 'angles_athwartship_e'):
                        # Convert from electrical angles to indexed.
                        angles_athwart_data = (angles_athwart_data / raw_data.INDEX2ELEC).astype('uint8')
                        angles_along_data = (angles_along_data / raw_data.INDEX2ELEC).astype('uint8')
                        dgram['angle'] = np.column_stack((angles_athwart_data,angles_along_data))

                        #  set the angles bit in the datatype value
                        datatype = datatype | (1 << 1)

                    # If we have power data, we have to convert it back to indexed
                    if hasattr(dg_objects[idx], 'power'):
                        dgram['power'] = (power_data / raw_data.INDEX2POWER).astype('int16')

                        #  set the power bit in the datatype value
                        datatype = datatype | (1 << 0)

                    # If we have complex data, we have to convert to non-complex for writing
                    if hasattr(dg_objects[idx], 'complex'):

                        # Determine the precision we'll use to write the data
                        if reduce_complex_precision:
                            # reduced complex
                            complex_dtype = np.float16
                            datatype = datatype | (1 << 2)
                        else:
                            # non-reduced complex
                            complex_dtype = dg_objects[idx].file_complex_dtype
                            datatype = datatype | (1 << 3)

                        # Set the number of complex samples in the datatype
                        n_complex = complex_data.shape[1]
                        datatype = datatype | (n_complex << 8)

                        # Now pack the complex data - use a view to transform complex64
                        # to the output type. Reshape to interleave sample data.
                        dgram['complex'] = complex_data.view(complex_dtype)
                        dgram['complex'].shape = (dgram['count'] * 2 * n_complex)

                    # Set the datatype
                    dgram['data_type'] = datatype

                    #  write it
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY[dg_type[idx]].to_string(dgram))

                elif dg_type[idx] == 'TAG':
                    #  create the base datagram dict
                    dgram = new_datagram(dg_times[idx], dg_type[idx],
                            DGRAM_VERSION_KEY[dg_type[idx]])

                    # Set the annotation text datagram value
                    dgram['text'] = dg_objects[idx].text[dg_obj_idx[idx]]

                    #  write it
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY[dg_type[idx]].to_string(dgram))

                elif dg_type[idx] == 'NME':
                    #  create the base datagram dict
                    dgram = new_datagram(dg_times[idx], dg_type[idx],
                            DGRAM_VERSION_KEY[dg_type[idx]])

                    # Set the NME0 datagram values
                    dgram['nmea_string'] = dg_objects[idx].raw_datagrams[dg_obj_idx[idx]]

                    #  write it
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY[dg_type[idx]].to_string(dgram))

                elif dg_type[idx] == 'MRU':
                    #  create the base datagram dict
                    dgram = new_datagram(dg_times[idx], dg_type[idx],
                            DGRAM_VERSION_KEY[dg_type[idx]])
                    # Assign the datagram values
                    dgram['heave'] = dg_objects[idx].heave[dg_obj_idx[idx]]
                    dgram['pitch'] = dg_objects[idx].pitch[dg_obj_idx[idx]]
                    dgram['roll'] = dg_objects[idx].roll[dg_obj_idx[idx]]
                    dgram['heading'] = dg_objects[idx].heading[dg_obj_idx[idx]]

                    #  write it
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY[dg_type[idx]].to_string(dgram))

                elif dg_type[idx] == 'ENV':
                    #  create the base datagram dict
                    dgram = new_datagram(dg_times[idx], 'XML', DGRAM_VERSION_KEY['XML'])
                    # Set the environment XML datagram values

                    dgram['environment'] = dg_objects[idx]
                    dgram['subtype'] = 'environment'

                    #  write it
                    bytes_written += raw_fid.write(DGRAM_PARSE_KEY['XML'].to_string(dgram))

                else:
                    #  we should never encounter an unknown type, but raise exception if so
                    raise ValueError('Unknown datagram type encountered: ' + dg_type[idx] +
                            '. This should never happen...')

                #  call progress callback if supplied
                if (progress_callback):
                    #  determine the progress as percent.
                    pct_progress = round(datagrams_processed / n_datagrams * 100.)

                    #  call the callback when the percent changes
                    if cumulative_pct != pct_progress:
                        # Call the provided callback - the callback has 3 args,
                        # the first is the full path to the current file and the
                        # second is the percent written and the third is the bytes
                        # written.
                        progress_callback(outfile_name, pct_progress, bytes_written,
                                callback_ref)
                        cumulative_pct = pct_progress

                # Increment our datagram counter - do this after the progress callback
                # block to ensure we emit a 0 percent callback
                datagrams_processed += 1

            # Done writing this file - close it
            raw_fid.close()

        # Writing complete, return a list of files written
        return files_written


    def __str__(self):
        """
        Reimplemented string method that provides some basic info about the
        EK80 class instance's contents.

        Returns: Message to print when calling print method on EK80 class
        instance object.
        """

        # Print the class and address.
        msg = str(self.__class__) + " at " + str(hex(id(self))) + "\n"

        # Print some more info about the EK80 instance.
        if self.channel_ids:
            n_channels = len(self.channel_ids)
            if n_channels > 1:
                msg = msg + ("    EK80 object contains data from " + str(
                    n_channels) + " channels:\n")
            else:
                msg = msg + ("    EK80 object contains data from 1 channel:\n")

            for channel_id in self.channel_ids:
                for raw in self.raw_data[channel_id]:
                    msg = msg + ("        " + channel_id + " :: " + raw.data_type + " " + str(raw.shape) + "\n")
            msg = msg + ("    data start time: " + str(self.start_time) + "\n")
            msg = msg + ("      data end time: " + str(self.end_time) + "\n")
            msg = msg + ("    number of pings: " + str(self.end_ping -
                                                       self.start_ping + 1) + "\n")

        else:
            msg = msg + ("  EK80 object contains no data\n")

        return msg


class raw_data(ping_data):
    """
    the raw_data class contains a single channel's data extracted from a
    Kongsberg EK80 raw data file.
    """

    # Define some instrument specific constants.

    # Define constants used to specify the target resampling interval for the
    # power and angle conversion functions.  These values represent the
    # standard sampling intervals for EK60 hardware when operated with the
    # ER60 software as well as ES60/70 systems and the ME70(?).
    RESAMPLE_SHORTEST = 0
    RESAMPLE_16   = 0.000016
    RESAMPLE_32  = 0.000032
    RESAMPLE_64  = 0.000064
    RESAMPLE_128  = 0.000128
    RESAMPLE_256 = 0.000256
    RESAMPLE_512 = 0.000512
    RESAMPLE_1024 = 0.001024
    RESAMPLE_2048 = 0.002048
    RESAMPLE_LONGEST = 1

    # Create a constant to convert indexed power to power.
    INDEX2POWER = (10.0 * np.log10(2.0) / 256.0)

    # Create a constant to convert from indexed angles to electrical angles.
    INDEX2ELEC = 180.0 / 128.0

    # Specify the default transducer sector impedance in ohms. This value is
    # provided by Simrad
    ZTRANSDUCER = 75.0

    # Specify the default transceiver impedance in ohms. This value is
    # used when the transceiver impedance is not availabe in the data file.
    # Older versions of the EK80 configuration header did not include this
    # value. Earlier versions of the EK80 software assumed a nominal
    # impedance of 1000 omhs. This may or may not have changed to 5400 before
    # transceiver impedance was included in the data file so this may not
    # be correct for all older files.
    ZTRANSCEIVER = 1000


    def __init__(self, channel_id, n_pings=500, n_samples=-1,
                 rolling=False, store_power=True,
                 store_angles=True, store_complex=True,
                 max_sample_number=None):
        """Creates a new, empty raw_data object.

        The raw_data class stores raw echosounder data from a single channel
        recorded using the Kongsberg EK80 application. This class supports
        both EK60 GPTs running on EK80 software as well as EK80 WBT's operated
        using the EK80 software.

        NOTE: power is *always* stored in log form. If you manipulate power
              values directly, make sure they are stored in log form.

        The data arrays are not created upon instantiation. They will be created
        when the first ping is appended using the append_ping method.

        If rolling is True, and both the n_pings and n_samples arguments are
        provided, arrays of size (n_pings, n_samples) are created upon
        instantiation. Otherwise, the data arrays are created when the first
        data are added.


        Args:
            channel_id (str): The channel ID of channel whose data are stored
                in this raw_data instance.
            n_pings (int): Sets the fixed "width" of the arrays for rolling
                arrays and if the object is not using rolling arrays it defines
                the "width" of the allocation chunks when appending data.
            n_samples (int): Sets the number of samples (rows) of the
                arrays. Default value is -1 samples which means the arrays
                will automatically resize to hold all of the sample data.
            rolling (bool): True = arrays have fixed sizes of n_pings set when
                the class is instantiated. When the n_pings + 1 ping is added
                the array is rolled left, dropping oldest ping and adding newest.
            store_power (bool): Boolean to control whether power data are
                stored in this raw_data object.
            store_angles (bool): Boolean to control whether angle data are
                stored in this raw_data object.
            store_complex (bool): Boolean to control whether complex data are
                stored in this raw_data object.
            max_sample_number (int): Integer specifying the maximum number of
                samples that will be stored in this instance's data arrays.
        """
        super(raw_data, self).__init__()

        # Specify if data array size is fixed and the array data is rolled left
        # if the array fills up (True) or if the arrays are expanded when
        # necessary to hold additional data (False).
        self.rolling_array = bool(rolling)

        # If rolling is set, ensure we have been passed n_samples
        if self.rolling_array and n_samples < 1:
            raise ValueError('The n_samples argument must be defined and ' +
                    'greater than 0 when rolling == True.')

        # The channel ID is the unique identifier of the channel stored in
        # the object.
        self.channel_id = channel_id

        # Specify the horizontal size (columns) of the array allocation size.
        self.chunk_width = n_pings

        # Keep note if we should store the power, angle, or complex data.
        self.store_power = bool(store_power)
        self.store_angles = bool(store_angles)
        self.store_complex = bool(store_complex)

        # Max_sample_number can be set to an integer specifying the maximum
        # number of samples that will be stored in the sample data arrays.
        # Samples beyond this will be discarded.
        self.max_sample_number = max_sample_number

        # data_type will be set to a string describing the type of sample data
        # stored. This value is an empty string until the first raw datagram
        # is added. At that point the data_type is set. A raw_data object can
        # only store 1 type of data. The data typse are:
        #
        #   angle - contains angle data only
        #   power - contains power data
        #   power/angle - contains power and angle data
        #   complex-CW - contains complex CW data
        #   complex-FM - contains complex FM data
        self.data_type = ''

        # The file_complex_dtype is the data type the complex data are stored
        # as. This attribute is set when the first ping is appended and is
        # ignored for reduced data.
        self.file_complex_dtype = np.float32

        # transceiver_type stores the hardware identifier of the transceiver
        # used to collect the data. This is set when reading the file.
        # I believe that the types are:
        #
        #   GPT - Ex60 General Purpose Transceiver
        #   WBT - Ex80 Wide Band Transceiver
        #   WBT MINI - A WBT meets Dr. Shrinker
        #   WBT TUBE - A WBT in a tube
        #   WBT HP -
        #   WBT LF -
        #   WBAT - Wide Band Autonomous Transceiver
        #   SBT -
        self.transceiver_type = None

        # motion_data stores a reference to the motion_data object created by
        # the EK80 class when data are read. The motion_data object stores the
        # heading, pitch, roll, and heave data recorded by the EK80 system if
        # the motion sensor was installed.
        self.motion_data = None

        # nmea_data stores a reference to the nmea_data object created by
        # the EK80 class when data are read. The nmea_data object stores the
        # asyncronous NMEA-0183 data input into the EK80 system such as GPS,
        # Gyro, vessel distance log, etc.
        self.nmea_data = None

        # Data_attributes is an internal list that contains the names of all
        # of the class's "data" properties. The echolab2 package uses this
        # attribute to generalize various functions that manipulate these
        # data.  Here we *extend* the list that is defined in the parent class.
        self._data_attributes += ['configuration',
                                  'environment',
                                  'filters',
                                  'channel_mode',
                                  'pulse_form',
                                  'pulse_duration',
                                  'sample_interval',
                                  'slope',
                                  'transmit_power',
                                  'sample_count',
                                  'sample_offset']

        #  create a list that stores the scalar object attributes
        self._obj_attributes = ['rolling_array',
                                'chunk_width',
                                'store_power',
                                'store_angles',
                                'store_complex',
                                'max_sample_number',
                                'data_type',
                                'transceiver_type',
                                'file_complex_dtype',
                                'motion_data',
                                'nmea_data']


    def empty_like(self, n_pings, no_data=False):
        """Returns raw_data object with data arrays filled with NaNs.

        The raw_data object has the same general characteristics of "this"
        object, but with data arrays filled with NaNs.

        Args:
            n_pings (int): Set n_pings to an integer specifying the number of
                pings in the new object. The vertical axis (both number of
                samples and depth/range values) will be the same as this object.
        """

        # Create an instance of echolab2.EK80.raw_data and set the same basic
        # properties as this object.  Return the empty processed_data object.
        empty_obj = raw_data(self.channel_id, n_pings=n_pings,
                n_samples=self.n_samples, rolling=self.rolling_array,
                chunk_width=n_pings, store_power=self.store_power,
                store_angles=self.store_angles, store_complex=self.store_complex,
                max_sample_number=self.max_sample_number)

        return self._like(empty_obj, n_pings, np.nan, empty_times=True,
                no_data=no_data)


    def copy(self):
        """creates a deep copy of this object."""

        # Create an empty raw_data object with the same basic props as this object.
        rd_copy = raw_data(self.channel_id)

        # Call the parent _copy helper method and return the result.
        return self._copy(rd_copy)


    def insert(self, obj_to_insert, ping_number=None, ping_time=None,
               insert_after=True, index_array=None, force=False):
        """Inserts data from one raw_data object into another. Pings within the
        obj_to_insert can be inserted as a block, or they can be inserted
        individually.

        Args:
            obj_to_insert: An instance of echolab2.EK80.raw_data that contains
                the data to insert.If obj_to_insert is None, pings are inserted
                with all fields set to NaN.

            ping_number, ping_time, and index_array are exclusive. Only set 1.

            ping_number (int): Set to an integer indicating the ping number where the
                data should be inserted.
            ping_time (datetime):Set to an integer indicating the ping number where the
                data should be inserted.
            index_array (array):Set to a numpy array that is the same length
                as the raw_data object you're inserting where each element is
                an index into the array you're inserting into which maps each
                inserted ping into the raw_data object.
            insert_after (bool): Set to True to insert data after the ping/time/index
                specified.

        """
        # Determine how many pings we're inserting.
        if index_array is None:
            in_idx = self.get_indices(start_time=ping_time, end_time=ping_time,
                    start_ping=ping_number, end_ping=ping_number)[0]
            n_inserting = self.n_pings - in_idx
        else:
            n_inserting = index_array.shape[0]

        # When obj_to_insert is None, we automatically create a matching
        # object that contains no data (all NaNs).
        if obj_to_insert is None:
            obj_to_insert = self.empty_like(n_inserting)

        # Check that the data types are the same.
        if not isinstance(obj_to_insert, raw_data):
            raise TypeError('The object you are inserting must be an instance '
                    + 'of echolab2.EK80.raw_data')

        # We are now coexisting in harmony - call parent's insert.
        super(raw_data, self).insert(obj_to_insert, ping_number=ping_number,
                ping_time=ping_time, insert_after=insert_after,
                index_array=index_array)


    def append_ping(self, sample_datagram, config_params, environment_datagram,
            tx_parms, filters, start_sample=None, end_sample=None):
        """Adds a "pings" worth of data to the object.

        This method extracts data from the provided sample_datagram dict and
        inserts it into the data arrays. Managing the data array sizes is the
        bulk of what this method does:

        If the raw_data.rolling_array attribute is false, columns are added to
        the data arrays as needed. To reduce overhead, the arrays are extended in
        chunks, not on a ping by ping basis. If the recording range or the pulse
        length changes requiring additional rows to be added, the data arrays will be
        resized to accomodate the maximum number of samples being stored. Existing
        samples are padded with NaNs as required. This vertical resize does not
        occur in chunks and the data are copied each time samples are added.
        This can have significant performance impacts in specific cases and this
        method may need to be extended to support bulk allocation of the sample axis
        as well.

        If raw_data.rolling_array is true, the data arrays are not resized but
        the data within the arrays is "rolled" or shifted left and the column at
        index 0 is moved to index n_pings - 1. Additional samples are discarded
        and pings with fewer samples are padded with NaNs as required.

        This method is typically only called by the EK80 class when reading a raw file.

        Args:
            sample_datagram (dict): The dictionary containing the parsed sample datagram.
            config_datagram (dict): The dictionary containing the parsed XML configuration
                                    datagram that goes with this sample data.
            environment_datagram (dict): A dictionary containing the latest parsed XML
                                         environment datagram.
            tx_parms (dict): The dictionary containing the most recent XML parameter
                             datagram for this channel.
            filters (dict): A dictionary containing this channels filter coefficients
                            used by the EK80 to filter the received signal.
            start_sample (int): The starting sample to store in the object.
            end_sample (int): The ending sample to store in the object.
        """

        # Set some defaults
        complex_samps = -1
        power_samps = -1
        angle_samps = -1
        n_complex = 0
        max_data_samples = []

        # The contents of the first ping appended to a raw_data object defines
        # the data_type and determines how the data arrays will be created. Also,
        # since a raw_data object can only store onedata_type, we disable saving
        # of the other types.
        if self.n_pings == -1:

            # Set defaults
            create_complex = False
            create_power = False
            create_angles = False

            #  Determine what kind of data we've been given - complex or power/angle
            if sample_datagram['complex'] is not None:
                # This is a complex datagram. Store complex data only
                create_complex = True
                complex_samps = sample_datagram['complex'].shape[0]
                n_complex = sample_datagram['complex'].shape[1]
                self.store_power = False
                self.store_angles = False

                #  determine if this is an FM or CW file
                if tx_parms['pulse_form'] == 0:
                    # CW files will have the frequency parameter
                    self.data_type = 'complex-CW'
                    self._data_attributes += ['frequency']
                else:
                    # FM files will have the frequency_start and frequency_end parameters
                    self.data_type = 'complex-FM'
                    self._data_attributes += ['frequency_start', 'frequency_end']

                # Set the file's complex data type
                if sample_datagram['data_type'] & 0b1000:
                    self.file_complex_dtype = np.float32
                else:
                    self.file_complex_dtype = np.float16

                # For complex data, we set the sample array type to complex64
                # regardless of the file_complex_dtype since numpy doesn't
                # support the complex32 type.
                self.sample_dtype = np.complex64

            else:
                # This must be "reduced" data (power, angle, or power/angle datagram)
                # Check if either or both are provided
                if  sample_datagram['power'] is not None:
                    create_power = True
                    self.store_complex = False
                    power_samps = sample_datagram['power'].shape[0]
                    self.data_type = 'power'

                if sample_datagram['angle'] is not None:
                    create_angles = True
                    self.store_complex = False
                    angle_samps = sample_datagram['angle'].shape[0]
                    if create_power:
                        self.data_type = 'power/angle'
                    else:
                        self.store_power = False
                        self.data_type = 'angle'

                # At this point if we're power, we're not storing angle data
                if self.data_type == 'power':
                    self.store_angles = False

                # Set the "short" data type
                self.short_data_type = sample_datagram['data_type']

                # All reduced data is CW so add the frequency attribute
                self._data_attributes += ['frequency']

            #  determine the initial number of samples in our arrays
            if self.max_sample_number:
                n_samples = self.max_sample_number
            else:
                n_samples = max([angle_samps, power_samps, complex_samps])

            #  set the initial sample offset
            self.sample_offset = sample_datagram['offset']

            #  Initialize the data arrays
            self._create_arrays(self.chunk_width, n_samples, initialize=False,
                    create_power=create_power, create_angles=create_angles,
                    create_complex=create_complex, n_complex=n_complex)

            # Initialize the ping counter to indicate that our data arrays
            # have been allocated.
            self.n_pings  = 0

        #  determine the number of samples in this datagram as well
        # as the number of samples in our data array(s).
        if self.store_angles:
            angle_samps = sample_datagram['angle'].shape[0]
            max_data_samples.append(self.angles_alongship_e.shape[1])
            max_data_samples.append(self.angles_athwartship_e.shape[1])
        if self.store_power:
            power_samps = sample_datagram['power'].shape[0]
            max_data_samples.append(self.power.shape[1])
        if self.store_complex:
            complex_samps = sample_datagram['complex'].shape[0]
            n_complex = sample_datagram['complex'].shape[1]
            max_data_samples.append(self.complex.shape[1])

            # Check to ensure the number of sectors hasn't changed.
            # This should never happen since I am handling this in the
            # EK80 class but we'll keep the check here anyways.
            if self.complex.shape[2] != n_complex:
                raise ValueError("The number of complex values changed after object " +
                        "creation. Why didn't the channel ID change??")

        max_data_samples = max(max_data_samples)
        max_new_samples = max([power_samps, angle_samps, complex_samps])

        # Check if we need to truncate the sample data.
        if self.max_sample_number and (max_new_samples > self.max_sample_number):
            max_new_samples = self.max_sample_number
            if self.store_angles:
                sample_datagram['angle'] = \
                        sample_datagram['angle'][0:self.max_sample_number]
            if self.store_power:
                sample_datagram['power'] = \
                        sample_datagram['power'][0:self.max_sample_number]
            if self.store_complex:
                sample_datagram['complex'] = \
                        sample_datagram['complex'][0:self.max_sample_number:]

        # Create 2 variables to store our current array size.
        ping_dims = self.ping_time.size
        sample_dims = max_data_samples

        # Check if we need to re-size or roll our data arrays.
        if self.rolling_array == False:
            # Check if we need to resize our data arrays.
            ping_resize = False
            sample_resize = False

            # Check the ping dimension.
            if self.n_pings == ping_dims:
                # Need to resize the ping dimension.
                ping_resize = True
                # Calculate the new ping dimension.
                ping_dims = ping_dims + self.chunk_width

            # Check the samples dimension.
            if max_new_samples > max_data_samples:
                # Need to resize the samples dimension.
                sample_resize = True
                # Calculate the new samples dimension.
                sample_dims = max_new_samples

            # Determine if we resize.
            if ping_resize or sample_resize:
                self.resize(ping_dims, sample_dims)

            # Get an index into the data arrays for this ping and increment
            # our ping counter.
            this_ping = self.n_pings
            self.n_pings += 1

        else:
            # Check if we need to roll.
            if self.n_pings == ping_dims - 1:
                # When a rolling array is "filled" we stop incrementing the
                # ping counter and repeatedly append pings to the last ping
                # index in the array.
                this_ping = self.n_pings

                # Roll our array 1 ping.
                self._roll_arrays(1)

        # Update the channel configuration dict's end_* values
        config_params['end_ping'] = self.n_pings
        config_params['end_time'] = sample_datagram['timestamp']

        # Insert the config, environment, and filter object references for this ping.
        self.configuration[this_ping] = config_params
        self.environment[this_ping] = environment_datagram
        self.filters[this_ping] = filters

        # Now insert the data into our numpy arrays.
        self.ping_time[this_ping] = sample_datagram['timestamp']
        self.channel_mode[this_ping] = tx_parms['channel_mode']
        self.pulse_form[this_ping] = tx_parms['pulse_form']
        if tx_parms['pulse_form'] == 0:
            # CW files will have the freuqency parameter
            self.frequency[this_ping] = tx_parms['frequency']
        else:
            # FM files will have the frequency_start and frequency_end parameters
            self.frequency_start[this_ping] = tx_parms['frequency_start']
            self.frequency_end[this_ping] = tx_parms['frequency_end']
        self.pulse_duration[this_ping] = tx_parms['pulse_duration']
        self.sample_interval[this_ping] = tx_parms['sample_interval']
        self.slope[this_ping] = tx_parms['slope']
        self.transmit_power[this_ping] = tx_parms['transmit_power']

        # Update sample count and sample offset values
        if start_sample:
            self.sample_offset[this_ping] = start_sample + sample_datagram['offset']

            if end_sample:
                self.sample_count[this_ping] = end_sample - start_sample + 1
            else:
                self.sample_count[this_ping] = sample_datagram['count'] - \
                                               start_sample
        else:
            self.sample_offset[this_ping] = sample_datagram['offset']
            start_sample = 0
            if end_sample:
                self.sample_count[this_ping] = end_sample + 1
            else:
                self.sample_count[this_ping] = sample_datagram['count']
                end_sample = sample_datagram['count']

        # Now store the "sample" data.

        # Check if we need to store complex data.
        if self.store_complex:

            # Get the subset of samples we're storing.
            complex = sample_datagram['complex'][start_sample:self.sample_count[
                this_ping],:]

            # Check if we need to pad or trim our sample data.
            sample_pad = sample_dims - complex.shape[0]
            if sample_pad > 0:
                # The data array has more samples than this datagram - we
                # need to pad the datagram.
                self.complex[this_ping,:,:] = np.pad(complex,((0,sample_pad),(0,0)),
                        'constant', constant_values=np.nan)
            elif sample_pad < 0:
                # The data array has fewer samples than this datagram - we
                # need to trim the datagram.
                self.complex[this_ping,:,:] = complex[0:sample_pad,:]
            else:
                # The array has the same number of samples.
                self.complex[this_ping,:,:] = complex

        # Check if we need to store power data.
        if self.store_power:

            # Get the subset of samples we're storing.
            power = sample_datagram['power'][start_sample:self.sample_count[this_ping]]

            # Convert the indexed power data to power dB.
            power = power.astype(self.sample_dtype) * self.INDEX2POWER

            # Check if we need to pad or trim our sample data.
            sample_pad = sample_dims - power.shape[0]
            if sample_pad > 0:
                # The data array has more samples than this datagram - we
                # need to pad the datagram.
                self.power[this_ping,:] = np.pad(power,(0,sample_pad),
                        'constant', constant_values=np.nan)
            elif sample_pad < 0:
                # The data array has fewer samples than this datagram - we
                # need to trim the datagram.
                self.power[this_ping,:] = power[0:sample_pad]
            else:
                # The array has the same number of samples.
                self.power[this_ping,:] = power

        # Check if we need to store angle data.
        if self.store_angles:

            # Convert from indexed to electrical angles.
            alongship_e = sample_datagram['angle'][start_sample:self.sample_count[this_ping], 1].astype(self.sample_dtype)
            alongship_e *= self.INDEX2ELEC
            athwartship_e = sample_datagram['angle'][start_sample:self.sample_count[this_ping], 0].astype(self.sample_dtype)
            athwartship_e *= self.INDEX2ELEC

            # Check if we need to pad or trim our sample data.
            sample_pad = sample_dims - athwartship_e.shape[0]
            if sample_pad > 0:
                # The data array has more samples than this datagram - we
                # need to pad the datagram
                self.angles_alongship_e[this_ping,:] = np.pad(alongship_e,
                        (0,sample_pad), 'constant', constant_values=np.nan)
                self.angles_athwartship_e[this_ping,:] = np.pad(
                    athwartship_e,(0,sample_pad), 'constant', constant_values=np.nan)
            elif sample_pad < 0:
                # The data array has fewer samples than this datagram - we
                # need to trim the datagram.
                self.angles_alongship_e[this_ping,:] = alongship_e[0:sample_pad]
                self.angles_athwartship_e[this_ping,:] = athwartship_e[0:sample_pad]
            else:
                # The array has the same number of samples.
                self.angles_alongship_e[this_ping,:] = alongship_e
                self.angles_athwartship_e[this_ping,:] = athwartship_e


    def get_calibration(self, **kwargs):
        """Returns a calibration object populated from the data contained in this
        raw_data object.

        """

        cal_obj = ek80_calibration(**kwargs)
        cal_obj.from_raw_data(self)

        return cal_obj


    def get_power(self, calibration=None, **kwargs):
        """Returns a processed data object that contains the power data.

        This method performs all of the required transformations to place the
        raw power data into a rectangular array where all samples share the same
        thickness and are correctly arranged relative to each other.

        This process happens in 3 steps:

                Data are resampled so all samples have the same thickness.
                Data are shifted vertically to account for the sample offsets.
                Data are then regridded to a fixed time, range grid.

        Each step is performed only when required. Calls to this method will
        return much faster if the raw data share the same sample thickness,
        offset and sound speed.

        If calibration is set to an instance of EK80.ek80_ calibration the
        values in that object (if set) will be used when performing the
        transformations required to return the results. If the required
        parameters are not set in the calibration object or if no object is
        provided, this method will extract these parameters from the raw file
        data.

        Args:

            resample_interval (float): Set this to a float specifying the
                sampling interval (in seconds) used when generating the
                vertical axis for the return data. If the raw data sampling
                interval is different than the specified interval, the raw
                data will be resampled at the specified rate. 0 and 1 have
                special meaning. If set to 0 or 1, the data will only be resampled
                if the sampling interval changes. If it does change, when
                set to 0, the data will be resampled to the shortest sampling
                interval present in the data. If set to 1, it will be resampled
                to the longest interval present in the data.

                With GPTs, the sampling intervals were all multiples of 2 and
                data can be resampled by replicating or averaging samples.
                The sampling intervals in WBT hardware do not follow this
                pattern so data must be interpolated. Interpolating will cause
                aliasing which can be significant when downsampling data.
                Use with care.

                The following constants are defined in the class:

                    RESAMPLE_SHORTEST = 0 (upsample data to shortest sample interval)
                    RESAMPLE_16   = 0.000016
                    RESAMPLE_32  = 0.000032
                    RESAMPLE_64  = 0.000064
                    RESAMPLE_128  = 0.000128
                    RESAMPLE_256 = 0.000256
                    RESAMPLE_512 = 0.000512
                    RESAMPLE_1024 = 0.001024
                    RESAMPLE_2048 = 0.002048
                    RESAMPLE_LONGEST = 1 (downsample data to longest sample interval)

                Default: RESAMPLE_SHORTEST

            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you want to use when transforming to Sv/sv. If no calibration
                object is provided, the values will be extracted from the raw
                data.

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to convert. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to convert. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the first ping is set as the start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the last ping is set as the end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored.

            time_order (bool): Set to True to return data in time order. If
                False, data will be returned in the order it was read.
                Default: True

            Zer (float): Specify the WBT receiver nominal impedance in ohms.
                The default is to use the receiver nominal impedance from the
                raw data if available. If the data were recorded using a version
                of EK80 that didn't store that value, 5400 ohms is used. Some
                older WBT hardware had a nominal impedance of 1000 ohms. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

            Zet (float): Specify the transducer nominal impedance in ohms.
                The default is 75 ohms. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

        Returns:
            A processed_data object containing power data.

        """
        # This method is identical to _get_power except we don't return the
        # index array.
        p_data, _ = self._get_power(calibration=calibration, **kwargs)

        return p_data


    def _complex_to_power(self, calibration, return_indices, return_angles=False,
            pulse_compress=True, **kwargs):
        '''_complex_to_power converts the complex data in the processed_data
        object p_data to power. It will optionally compute and return electrical andgle
        data.

        The methods below was derived from code provided by Lars Nonboe Andersen
        (Kongsberg Maritime) and information in the USA–Norway EK80 Workshop Report:

        Demer, D. A., Andersen, L. N., Bassett, C., Berger, L., Chu, D., Condiotty, J., Cutter, G.
        R., et al. 2017. 2016 USA–Norway EK80 Workshop Report: Evaluation of a wideband
        echosounder for fisheries and marine ecosystem science. ICES Cooperative Research
        Report No. 336. 69 pp. http://doi.org/10.17895/ices.pub.2318

        '''

        # extract the fast keyword. If present - The fast keyword tells the pulse
        # compression function to assume the Tx params for all FM pings are constant
        # and thus a single Tx signal is coputed and used for all conversions.
        fast = kwargs.get('fast',False)

        if pulse_compress:
            # Pulse compress (this funtion has no effect on CW data)
            p_data = simrad_signal_proc.pulse_compression(self, calibration,
                return_indices=return_indices, fast=fast)

        # get the impedance values we need to convert to power
        Zet = kwargs.pop('Zet', None)
        if Zet is None:
            # default transducer impedance (Demer et. al 2017 ICRR #336)
            Zet = raw_data.ZTRANSDUCER
        Zer = kwargs.pop('Zer', None)
        if Zer is None:
            if calibration.impedance and not np.isnan(calibration.impedance):
                Zer = calibration.impedance
            else:
                # default receiver impedance (Demer et. al 2017 ICRR #336)
                Zer = raw_data.ZTRANSCEIVER

        # Determine the number of transducer sectors
        n_sectors = p_data.shape[2]

        # Check if we're supposed to return angles
        if return_angles:

            #  get the unique beam type for all pings
            beam_type = np.unique(calibration.get_parameter(self,
                    'transducer_beam_type',return_indices))

            #  make sure every ping has the same type to keep it simple.
            #  You can implement something more complete if you need it.
            if beam_type.size > 1:
                raise NotImplementedError('Angle conversion for arrays of mixed beam ' +
                    'types is not suppored at this time. Beam types in data:' +
                    ','.join(list(beam_type)))
            else:
                beam_type = beam_type[0]

            # Compute electrical angles based on the beam type
            if beam_type in [1,17,49,65,81]:

                if beam_type == 1:
                    #  transducer with 4 quadrants

                    # average the quadrant pairs
                    yfore = np.sum(p_data[:,:,2:4], axis=2) / 2
                    yaft = np.sum(p_data[:,:,0:2], axis=2) / 2
                    ystar = (p_data[:,:,0] + p_data[:,:,3]) / 2
                    yport = np.sum(p_data[:,:,1:3], axis=2) / 2

                    # Compute the electrical angles
                    p_data_alongship = np.angle(yfore * np.conj(yaft), deg=True)
                    p_data_athwartship = np.angle(ystar * np.conj(yport), deg=True)

                elif beam_type in [17,49,65,81]:
                    #  transducers with 3 sectors, or 3 sectors and a center element

                    if beam_type == 17:
                        # no averaging with 3 sector transducers
                        ystar = p_data[:,:,0]
                        yport = p_data[:,:,1]
                        yfore = p_data[:,:,2]
                    else:
                        # Transducer with 3 sectors and center element.
                        # Average the sectors with center
                        ystar = (p_data[:,:,0] + p_data[:,:,3]) / 2.0
                        yport = (p_data[:,:,1] + p_data[:,:,3]) / 2.0
                        yfore = (p_data[:,:,2] + p_data[:,:,3]) / 2.0

                    # Compute the electrical angles
                    x = np.angle(yfore * np.conj(ystar), deg=True)
                    y = np.angle(yfore * np.conj(yport), deg=True)
                    p_data_alongship = (x + y) / np.sqrt(3)
                    p_data_athwartship = y - x

            else:
                # this is an unknown/unsupported beam type
                p_data_alongship = None
                p_data_athwartship = None

        # Compute power
        Ur_t = np.mean(p_data, axis=2)
        vrsplit = (Zer + Zet) / Zer
        Per_t = (n_sectors * (np.abs(Ur_t) / (2 * np.sqrt(2)))**2 *
                vrsplit**2 * 1 / Zet)
        Per_t[Per_t == 0] = np.nan

        # convert to log units
        p_data = 10 * np.log10(Per_t)

        if return_angles:
            return (p_data, p_data_alongship, p_data_athwartship)
        else:
            return p_data


    def _get_power(self, calibration=None, **kwargs):
        """Returns a processed data object that contains the power data and
        an index array.

        This method is identical to get_power except that it also returns an
        index array that maps the pings in the processed_data object to the
        same pings in the "this" object. This is used internally.

        Args:
            **kwargs (dict): A keyworded argument list.

        Returns:
            The processed data object, p_data, and an index array of pings,
            return_indices.
        """

        # Check if user provided a cal object
        if calibration is None:
            # No - create one populated from raw data
            calibration = self.get_calibration()

        # Call the _get_sample_data method requesting the appropriate sample attribute.
        if hasattr(self, 'power') or hasattr(self, 'complex'):

            # get power data in a processed_data object
            p_data, return_indices = self._get_sample_data('power',
                    calibration=calibration, **kwargs)

        else:
            raise AttributeError('Raw data object does not contain power or ' +
                    'complex data required to return power.')

        # Set the data type.
        p_data.data_type = 'power'

        # Set the is_log attribute and return it.
        p_data.is_log = True

        return p_data, return_indices


    def get_sv(self, **kwargs):
        """Gets sv data.

        This is a convenience method which simply calls get_Sv and forces
        the linear keyword to True.

        Args:
            See getSv for arguments.

        Returns:
            A processed_data object containing sv.

        """

        # Remove the linear keyword.
        kwargs.pop('linear', None)

        # Call get_Sp forcing linear to True.
        return self.get_Sv(linear=True, **kwargs)


    def get_Sv(self, calibration=None, linear=False, tvg_correction=True,
               return_depth=False, clear_cache=True, **kwargs):
        """Gets Sv data

        This method returns a processed_data object containing Sv or sv data.

        This method performs all of the required transformations to place the
        raw power data into a rectangular array where all samples share the same
        thickness and are correctly arranged relative to each other. It then
        computes Sv/sv as follows:

            CW:

            Sv = power + 20 log10(Range) + (2 *  alpha * Range) - (10 * ...
                log10((TransmitPower * (10^(Gain/10))^2 * lambda^2 * ...
                c * tau * 10^(psi/10)) / (32 * pi^2)) - (2 * SaCorrection)


            FM:

            Sv = power + 20 log10(Range) + (2 *  alpha * Range) - (10 * ...
                log10((TransmitPower * (10^(Gain/10))^2 * lambda^2 * ...
                c * tau * 10^(psi/10)) / (32 * pi^2))


        Args:
            resample_interval (float): Set this to a float specifying the
                sampling interval (in seconds) used when generating the
                vertical axis for the return data. If the raw data sampling
                interval is different than the specified interval, the raw
                data will be resampled at the specified rate. 0 and 1 have
                special meaning. If set to 0 or 1, the data will only be resampled
                if the sampling interval changes. If it does change, when
                set to 0, the data will be resampled to the shortest sampling
                interval present in the data. If set to 1, it will be resampled
                to the longest interval present in the data.

                With GPTs, the sampling intervals were all multiples of 2 and
                data can be resampled by replicating or averaging samples.
                The sampling intervals in WBT hardware do not follow this
                pattern so data must be interpolated. Interpolating will cause
                aliasing which can be significant when downsampling data.
                Use with care.

                The following constants are defined in the class:

                    RESAMPLE_SHORTEST = 0 (upsample data to shortest sample interval)
                    RESAMPLE_16   = 0.000016
                    RESAMPLE_32  = 0.000032
                    RESAMPLE_64  = 0.000064
                    RESAMPLE_128  = 0.000128
                    RESAMPLE_256 = 0.000256
                    RESAMPLE_512 = 0.000512
                    RESAMPLE_1024 = 0.001024
                    RESAMPLE_2048 = 0.002048
                    RESAMPLE_LONGEST = 1 (downsample data to longest sample interval)

                Default: RESAMPLE_SHORTEST

            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

                Default: None (return all pings)

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you want to use when transforming to Sv/sv. If no calibration
                object is provided, the values will be extracted from the raw
                data.

                Default: None

            clear_cache (bool): Set to True to clear out intermediate data cached
                in the calibration object during conversion. In order to speed up
                conversions, the Tx signal and effective pulse length are cached when
                first computed and the values are reused during subsequent computations
                during conversion. In order to ensure that the correct data are used
                during subsequent calls to a conversion routine, these data are
                discarded at the end of the conversion. Set this keyword to False to
                keep the cached data. It is your responsibility to use this cached data
                wisely.

                Default: True

            linear (bool): Set to True if getting "sv" data
                Default: False

            tvg_correction (bool): Set to True to apply TVG range correction.
                Typically you want to leave this at True.
                Default: True

            return_depth (bool): Set to True to return a processed_data object
                with a depth axis. When False, the processed_data object has
                a range axis.
                Default: False

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to convert. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to convert. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the first ping is set as the start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the last ping is set as the end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored.

            time_order (bool): Set to True to return data in time order. If
                False, data will be returned in the order it was read.
                Default: True

            Zer (float): Specify the WBT receiver nominal impedance in ohms.
                The default is to use the receiver nominal impedance from the
                raw data if available. If the data were recorded using a version
                of EK80 that didn't store that value, 1000 ohms is used. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

            Zet (float): Specify the transducer nominal impedance in ohms.
                The default is 75 ohms. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

        Returns:
            A processed_data object containing Sv (or sv if linear is True).

        """

        # Check if user provided a cal object
        if calibration is None:
            # No - get one populated from raw data
            calibration = self.get_calibration()

        # Get the power data - this step also resamples and arranges the raw data.
        p_data, return_indices = self._get_power(calibration=calibration, **kwargs)

        # Set the data type and is_log attribute.
        if linear:
            attribute_name = 'sv'
            p_data.is_log = False

        else:
            attribute_name = 'Sv'
            p_data.is_log = True
        p_data.data_type = attribute_name

        # Convert power to Sv/sv.
        sv_data = self._convert_power(p_data, calibration, attribute_name,
                linear, return_indices, tvg_correction)

        # Set the data attribute in the processed_data object.
        p_data.data = sv_data

        # Check if we need to convert to depth
        if return_depth:
            p_data.to_depth()

        # check if we're clearing the cached intermediate data in the cal object
        if clear_cache:
            #  yes - check for and delete cached
            if hasattr(calibration, '_tx_signal'):
                delattr(calibration, '_tx_signal')
            if hasattr(calibration, '_tau_eff'):
                delattr(calibration, '_tau_eff')

        return p_data


    def get_sp(self, **kwargs):
        """Gets sp data.

        This method returns a processed_data object containing sp data. This is
        a convenience method which simply calls get_Sp and forces the linear
        keyword to True.

        See get_Sp for more detail.

        Args:
            See get_Sp for argument descriptions.

        Returns:
            returns a processed_data object containing sp
        """

        # Remove the linear keyword.
        kwargs.pop('linear', None)

        # Call get_Sp, forcing linear to True.
        return self.get_Sp(linear=True, **kwargs)


    def get_Sp(self,  calibration=None, linear=False, tvg_correction=False,
            return_depth=False, clear_cache=True, **kwargs):
        """Gets Sp data.

        This method returns a processed_data object containing Sp or sp data.

        This method performs all of the required transformations to place the
        raw power data into a rectangular array where all samples share the same
        thickness and are correctly arranged relative to each other. It then
        computes Sp as follows:

             Sp = power + 40 * log10(Range) + (2 *  alpha * Range) - (10
             * ... log10((TransmitPower * (10^(gain/10))^2 * lambda^2) / (16 *
             pi^2)))

        Args:
            resample_interval (float): Set this to a float specifying the
                sampling interval (in seconds) used when generating the
                vertical axis for the return data. If the raw data sampling
                interval is different than the specified interval, the raw
                data will be resampled at the specified rate. 0 and 1 have
                special meaning. If set to 0 or 1, the data will only be resampled
                if the sampling interval changes. If it does change, when
                set to 0, the data will be resampled to the shortest sampling
                interval present in the data. If set to 1, it will be resampled
                to the longest interval present in the data.

                With GPTs, the sampling intervals were all multiples of 2 and
                data can be resampled by replicating or averaging samples.
                The sampling intervals in WBT hardware do not follow this
                pattern so data must be interpolated. Interpolating will cause
                aliasing which can be significant when downsampling data.
                Use with care.

                The following constants are defined in the class:

                    RESAMPLE_SHORTEST = 0 (upsample data to shortest sample interval)
                    RESAMPLE_16   = 0.000016
                    RESAMPLE_32  = 0.000032
                    RESAMPLE_64  = 0.000064
                    RESAMPLE_128  = 0.000128
                    RESAMPLE_256 = 0.000256
                    RESAMPLE_512 = 0.000512
                    RESAMPLE_1024 = 0.001024
                    RESAMPLE_2048 = 0.002048
                    RESAMPLE_LONGEST = 1 (downsample data to longest sample interval)

                Default: RESAMPLE_SHORTEST

            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you want to use when transforming to Sv/sv. If no calibration
                object is provided, the values will be extracted from the raw
                data.

            clear_cache (bool): Set to True to clear out intermediate data cached
                in the calibration object during conversion. In order to speed up
                conversions, the Tx signal and effective pulse length are cached when
                first computed and the values are reused during subsequent computations
                during conversion. In order to ensure that the correct data are used
                during subsequent calls to a conversion routine, these data are
                discarded at the end of the conversion. Set this keyword to False to
                keep the cached data. It is your responsibility to use this cached data
                wisely.

                Default: True

            linear (bool): Set to True if getting "sv" data
                Default: False

            tvg_correction (bool): Set to True to apply TVG range correction.
                By default, TVG range correction is not applied to the data. This
                results in output that is consistent with the Simrad "P" telegram
                and TS data exported from Echoview version 4.3 and later (prior
                versions applied the correction by default).

                If you intend to perform single target detections you must apply
                the TVG range correction at some point in your process. This can
                be done by either setting the tvgCorrection keyword of this function
                or it can be done as part of your single target detection routine.
                Default: False

            return_depth (bool): Set to True to return a processed_data object
                with a depth axis. When False, the processed_data object has
                a range axis.
                Default: False

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to convert. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to convert. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the first ping is set as the start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the last ping is set as the end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored.

            time_order (bool): Set to True to return data in time order. If
                False, data will be returned in the order it was read.
                Default: True

            Zer (float): Specify the WBT receiver nominal impedance in ohms.
                The default is to use the receiver nominal impedance from the
                raw data if available. If the data were recorded using a version
                of EK80 that didn't store that value, 5400 ohms is used. Some
                older WBT hardware had a nominal impedance of 1000 ohms. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

            Zet (float): Specify the transducer nominal impedance in ohms.
                The default is 75 ohms. See
                Demer et. al 2017 ICES Cooperative Research Report #336
                "Evaluation of a wideband echosounder for fisheries and marine
                ecosystem science" for details.

        Returns:
            A processed_data object containing Sp (or sp if linear is True).

        """

        # Check if user provided a cal object
        if calibration is None:
            # No - get one populated from raw data
            calibration = self.get_calibration()

        # Get the power data - this step also resamples and arranges the raw data.
        p_data, return_indices = self._get_power(calibration=calibration, **kwargs)

        # Set the data type.
        if linear:
            attribute_name = 'sp'
            p_data.is_log = False
        else:
            attribute_name = 'Sp'
            p_data.is_log = True
        p_data.data_type = attribute_name

        # Convert
        sp_data = self._convert_power(p_data, calibration, attribute_name,
                linear, return_indices, tvg_correction)

        # Set the data attribute in the processed_data object.
        p_data.data = sp_data

        # Check if we need to convert to depth or heave correct.
        if return_depth:
            p_data.to_depth()

        # check if we're clearing the cached intermediate data in the cal object
        if clear_cache:
            #  yes - check for and delete cached
            if hasattr(calibration, '_tx_signal'):
                delattr(calibration, '_tx_signal')
            if hasattr(calibration, '_tau_eff'):
                delattr(calibration, '_tau_eff')

        return p_data


    def get_physical_angles(self, calibration=None, clear_cache=True, **kwargs):
        """Gets the alongship and athwartship angle data.

        This method returns an tuple of processed data objects (alongship,
        athwartship) containing the physical angle data if raw angle data is
        available.

        For complex data types, this method only supports three sector transducers
        and four sector transducers with equal sized sectors (i.e. quadrants).
        This method does not support converting complex power to angles for
        four sector transducers with 3 outer sectors and a single center sector.

        This method performs all of the required transformations to place the
        raw electrical angle data into rectangular arrays where all samples
        share the same thickness and are correctly arranged relative to each other.
        It then transform the electrical angles into physical angles.

        Args:
            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you want to use when transforming to Sv/sv. If no calibration
                object is provided, the values will be extracted from the raw
                data.

            clear_cache (bool): Set to True to clear out intermediate data cached
                in the calibration object during conversion. In order to speed up
                conversions, the Tx signal and effective pulse length are cached when
                first computed and the values are reused during subsequent computations
                during conversion. In order to ensure that the correct data are used
                during subsequent calls to a conversion routine, these data are
                discarded at the end of the conversion. Set this keyword to False to
                keep the cached data. It is your responsibility to use this cached data
                wisely.

                Default: True

            return_depth (bool): Set to True to return a line object with a
                with a depth axis. When False, the line object has a range axis.
                Default: False

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to convert. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to convert. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the first ping is set as the start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the last ping is set as the end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored. If you provide an index array, both
            ping and time bounds will be ignored.

            time_order (bool): Set to True to return data in time order. If
                False, data will be returned in the order it was read.
                Default: True

        Returns:
            Two rocessed_data objects with alongship and athwartship angle data.
        """

        # Check if user provided a cal object
        if calibration is None:
            # No - get one populated from raw data
            calibration = self.get_calibration()

        # Get the electrical angles.
        alongship, athwartship, return_indices = \
                self.get_electrical_angles(calibration=calibration, **kwargs)

        # Get the calibration params required for angle conversion.
        cal_parms = {'angle_sensitivity_alongship':None,
                     'angle_sensitivity_athwartship':None,
                     'angle_offset_alongship':None,
                     'angle_offset_athwartship':None,
                     'frequency':None,
                     'frequency_start':None,
                     'frequency_end':None,
                     'transducer_frequency':None}

        # Next, iterate through the dict, calling the method to extract the
        # values for each parameter.
        for key in cal_parms:
            cal_parms[key] = calibration.get_parameter(self, key,
                    return_indices)

        # Adjust sensitivities for FM
        if cal_parms['frequency'] is None:
            fc = (cal_parms['frequency_start'] + cal_parms['frequency_end']) / 2
            fc /= cal_parms['transducer_frequency']
            sens_along = cal_parms['angle_sensitivity_alongship'] * fc
            sens_athwart = cal_parms['angle_sensitivity_athwartship'] * fc
        else:
            sens_along = cal_parms['angle_sensitivity_alongship']
            sens_athwart = cal_parms['angle_sensitivity_athwartship']

        # Compute the physical angles.
        alongship.data /= sens_along[:, np.newaxis]
        alongship.data -= cal_parms['angle_offset_alongship'][:, np.newaxis]
        athwartship.data /= sens_athwart[:, np.newaxis]
        athwartship.data -= cal_parms['angle_offset_athwartship'][:,np.newaxis]

        # Set the data types.
        alongship.data_type = 'angles_alongship'
        athwartship.data_type = 'angles_athwartship'

        # We do not need to convert to depth here since the electrical_angle
        # data will already have been converted to depth if requested.

        # check if we're clearing the cached intermediate data in the cal object
        if clear_cache:
            #  yes - check for and delete cached
            if hasattr(calibration, '_tx_signal'):
                delattr(calibration, '_tx_signal')
            if hasattr(calibration, '_tau_eff'):
                delattr(calibration, '_tau_eff')

        return alongship, athwartship


    def get_electrical_angles(self, return_depth=False, calibration=None, **kwargs):
        """Returns electrical angle data if available. Electrical angles are returned
        as an alongship, athwartship tuple of processed_data objects containing
        the angle data without the sensitivity and offset calibration data applied.

        Most users will want to call get_physical_angles()

        For complex data types, this method only supports three sector transducers
        and four sector transducers with equal sized sectors (i.e. quadrants).
        This method does not support converting complex power to angles for
        four sector transducers with 3 outer sectors and a single center sector.

        This method performs all of the required transformations to place the
        raw electrical angle data into rectangular arrays where all samples
        share the same thickness and are correctly arranged relative to each other.

        Args:
            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you want to use when transforming to Sv/sv. If no calibration
                object is provided, the values will be extracted from the raw
                data.

            return_depth (bool): Set to True to return a line object with a
                with a depth axis. When False, the line object has a range axis.
                Default: False

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to convert. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to convert. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the first ping is set as the start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All pings between the start and end ping will be
                returned. If set to None, the last ping is set as the end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored. If you provide an index array, both
            ping and time bounds will be ignored.

            time_order (bool): Set to True to return data in time order. If
                False, data will be returned in the order it was read.
                Default: True

        Returns:
            Two processed data objects containing the alongship and athwartship
            electrical angle data and an index array mapping pings to this object.
        """

        # Check if user provided a cal object
        if calibration is None:
            # No - get one populated from raw data
            calibration = self.get_calibration()

        # Call the _get_sample_data method requesting the 'angles_alongship_e'
        # sample attribute. The method will return a reference to a newly created
        # processed_data object.
        if (hasattr(self, 'angles_alongship_e') and hasattr(self,
                'angles_athwartship_e')) or hasattr(self, 'complex'):

            # Get the angle data
            alongship, athwartship, return_indices = self._get_sample_data('angles_e',
                    calibration=calibration, **kwargs)

        else:
            # We don't have complex nor electrical angle data required so
            # we can't do anything.
            raise AttributeError('raw_data object does not contain the sample ' +
                        'data required to return angle data.')

        # Set the data type.
        alongship.data_type = 'angles_alongship_e'
        athwartship.data_type = 'angles_athwartship_e'

        # Apply depth and/or heave correction
        if return_depth:
            alongship.to_depth()
            athwartship.to_depth()

        return alongship, athwartship, return_indices


    def get_bottom(self, calibration=None, return_indices=None, **kwargs):
        """Gets a echolab2 line object containing the sounder detected bottom
        depths.

        Use this method to get a line object representing bottom detections that
        have been read from EK80 .bot files. This method is only used if you
        have previously read .bot files using EK80.read_bot(). If you want to read
        bottom detections from .xyz files, you use the processing.line.read_xyz()
        method.

        The sounder detected bottom depths are computed using the sound speed
        setting at the time of recording. If you are applying a different sound
        speed setting via the calibration argument when getting the converted
        sample data, you must also pass the same calibration object to this method
        to ensure that the sounder detected bottom depths align with your sample
        data.

        Bottom detections are always in depth with heave corrections applied if
        heave data were input into and enabled in the EK80 software. You can use
        th

        Args:
            return_indices (np.array uint32): Set this to a numpy array that contains
                the index values to return in the processed data object. This can be
                used for more advanced anipulations where start/end ping/time are
                inadequate.

            calibration (EK80.ek80_calibration): Set to an instance of
                EK80.ek80_calibration containing the calibration parameters
                you used when transforming to Sv/sv. The shound speed value in
                the calibration object will be used to shift the bottom detections
                if the sound speed used during data recording is different than
                the sound speed specified in this object.

            start_time (datetime64): Set to a numpy datetime64 oject specifying
                the start time of the data to return. All data between the start
                and end time will be returned. If set to None, the start time is
                the first ping.
                Default: None

            end_time (datetime64): Set to a numpy datetime64 oject specifying
                the end time of the data to return. All data between the start
                and end time will be returned. If set to None, the end time is
                the last ping.
                Default: None

            start_ping (int): Set to an integer specifying the first ping number
                to return. All bottom detections between the start and end ping
                will be returned. If set to None, the first ping is set as the
                start ping.
                Default: None

            end_ping (int): Set to an integer specifying the end ping number
                to return. All bottom detections between the start and end ping
                will be returned. If set to None, the last ping is set as the
                end ping.
                Default: None

            Note that you can set a start/end time OR a start/end ping. If you
            set both, one will be ignored.


        Raises:
            ValueError: The return indices exceed the number of pings in
                the raw data object

        Returns:
            A line object containing the sounder detected bottom depths.

        """

        # Check if the user supplied an explicit list of indices to return.
        if isinstance(return_indices, np.ndarray):
            if max(return_indices) > self.ping_time.shape[0]:
                raise ValueError("One or more of the return indices provided " +
                        "exceeds the number of pings in the raw_data object")
        else:
            # Get an array of index values to return.
            return_indices = self.get_indices(**kwargs)

        # Check if user provided a cal object
        if calibration is None:
            # No - create an empty one - all cal values will come from the raw data
            calibration = ek80_calibration()

        # Extract the recorded sound velocity. Do this by creating an empty
        # cal object and getting the sound_speed parameter. This ensures
        # we get the sound speed as recorded.
        rec_sv_cal = ek80_calibration()
        sv_recorded = rec_sv_cal.get_parameter(self, 'sound_speed', return_indices)

        # Get the calibration params required for detected depth conversion.
        cal_parms = {'sound_speed':None,
                     'transducer_mounting':None,
                     'drop_keel_offset': None,
                     'transducer_offset_z': None,
                     'water_level_draft': None}

        # Next, iterate through the dict, calling the method to extract the
        # values for each parameter.
        for key in cal_parms:
            cal_parms[key] = calibration.get_parameter(self, key,
                    return_indices)

        # Check if we have to adjust the depth due to a change in sound speed.
        if np.all(np.isclose(sv_recorded, cal_parms['sound_speed'])):
            converted_depths = self.detected_bottom[return_indices]
        else:
            cf = cal_parms['sound_speed'].astype('float') / sv_recorded
            converted_depths = cf * self.detected_bottom[return_indices]

        # Create a line object to return with our adjusted data.
        bottom_line = line.line(ping_time=self.ping_time[return_indices],
                data=converted_depths, **kwargs)

        # Get the transducer offset attribute.
        xdcr_draft = self._get_transducer_offset(cal_parms)

        # Get the heave attribute if available and add to the xdcr draft
        _, heave_data = self.motion_data.interpolate(bottom_line, 'heave')
        if heave_data['heave'] is not None:
            xdcr_draft += heave_data['heave']

        # and then add the transducer_draft attribute to our line
        bottom_line.add_data_attribute('transducer_draft', xdcr_draft)

        return bottom_line


# The following 3 methods are probbaly more complicated than they need to be
# considering that normally mixed CW and FM data (or in the case of the
# get_frequency method mixed frequencies) can't occur in a normal raw file
# as the data would be in different channels.

    def is_fm(self, all=False):
        '''Convenience method that returns True if any of the pings contain
        FM data.
        '''

        # Check if we have any fm pings
        is_fm = self.pulse_form > 0
        if all:
            if np.all(is_fm):
                return True
            else:
                return False
        else:
            if np.any(is_fm):
                return True
            else:
                return False


    def is_cw(self, all=False):
        '''Convenience method that returns True if any of the pings contain
        CW data.
        '''
        # Check if we have any cw pings
        is_cw = self.pulse_form == 0
        if all:
            if np.all(is_cw):
                return True
            else:
                return False
        else:
            if np.any(is_cw):
                return True
            else:
                return False


    def get_frequency(self, unique=False):
        '''Convenience method that returns the frequency of the transmit signals
        of the data stored in the raw_data object. If the transmit signal is FM,
        the center frequency will be returned.

        Args:
            unique (bool): Set to True to return only the unique frequencies
        '''

        if self.is_fm(all=True):
            # All of the pings are FM
            frequency = (self.frequency_start + self.frequency_end) / 2.0
        elif self.is_cw(all=True):
            # All of the pings are CW
            frequency = self.frequency
        else:
            # The data are mixed so we have to do this ping by ping

            # I don't think this can this even happen?
            raise NotImplementedError

        if unique:
            frequency = np.unique(frequency)

        return frequency


    def _get_sample_data(self, property_name, calibration=None,
            resample_interval=RESAMPLE_SHORTEST, return_indices=None,
            **kwargs):
        """Internal method to get the specified sample data.

        This method returns a processed data object that contains the
        sample data from the property name provided. It performs all of the
        required transformations to place the data into a rectangular
        array where all samples in all pings share the same thickness and are
        correctly arranged relative to each other.

        This process happens in 3 steps:

                Data are resampled so all samples have the same sampling interval.
                Data are shifted vertically to account for the sample offsets.
                Data are then interpolated to a common sound velocity

        Each step is performed only when required. Calls to this method will
        return much faster if the raw data share the same sample thickness,
        offset and sound speed.

        If calibration is set to an instance of an EK80.calibration object
        the values in that object (if set) will be used when performing the
        transformations required to return the results. If the required
        parameters are not set in the calibration object or if no object is
        provided, this method will extract these parameters from the raw data.

        Note that with the introduction of support for EK80 complex data, this
        method was changed to handle complex->power and complex->angles
        conversions. The interface changed such that the property_name value
        is not a literal attrbute name, but a metatype specifying the type
        of data to return. The method will figure out what attributes to use
        to return the data.

        Args:
            property_name (str): The name of the data type to return. Valid
                values are:

                'power' - return power data.
                'angles_e' - return electrical angle data

                The available data will depend on how the data were collected
                and stored.

            calibration (EK80.calibration object): The calibration object where
                calibration data will be retrieved. If this is set to None,
                calibration data will be directly extracted from the raw

            resample_interval (float): The echosounder sampling interval used to
                define the sample thickness and range. If data was collected
                with a different sampling interval it will be resampled to
                the specified interval. The default behavior is to resample
                to the shortest sampling interval in the data. This value has
                no effect when all data share the same sampling interval.

            return_indices (array): A numpy array of indices to return.

        Raises:
            ValueError: Return indices exceeds the number of pings.
            AttributeError: The attribute name doesn't exist.

        Returns:
            The processed data object containing the sample data.
        """

        def get_range_vector(num_samples, sample_interval, sound_speed,
                             sample_offset):
            """
            get_range_vector returns a NON-CORRECTED range vector.
            """
            # Calculate the thickness of samples with this sound speed.
            thickness = sample_interval * sound_speed / 2.0
            # Calculate the range vector.
            range = (np.arange(0, num_samples) + sample_offset) * thickness

            return range

        # Check if the user supplied an explicit list of indices to return.
        if isinstance(return_indices, np.ndarray):
            if max(return_indices) > self.ping_time.shape[0]:
                raise ValueError("One or more of the return indices provided "
                        "exceeds the " + "number of pings in the " + "raw_data object")
        else:
            # Get an array of index values to return.
            return_indices = self.get_indices(**kwargs)

        # Create the processed_data object we will return.
        f = self.get_frequency(unique=True)[0]

        # Get references to the data we're operating on. With the
        # introduction of complex data where power and angle data are combined
        # in one data attribute, this method was modified to return a data
        # "metatype" since the power, angles_alongship_e, and angles_athwartship_e
        # attributes don't exist with EK80 complex data. We'll handle the
        # conversion of complex to power or angles as needed here and downstream
        # methods can more or less remain the same.

        # Here we determine what data to operate on given the type requested
        # and the data attributes we have.
        data_refs = []
        if hasattr(self, 'complex'):
            if property_name == 'power':
                # transform complex data to power
                raw_power = self._complex_to_power(calibration, return_indices, **kwargs)
                data_refs.append(raw_power)
            elif property_name == 'angles_e':
                # transform complex data to angles
                _, raw_along_e, raw_athwart_e = self._complex_to_power(calibration, return_indices,
                        return_angles=True, **kwargs)
                data_refs.append(raw_along_e)
                data_refs.append(raw_athwart_e)
        else:
            # No complex data - this is reduced data
            if property_name == 'power' and hasattr(self, 'power'):
                data_refs.append(getattr(self, property_name))
            elif (property_name == 'angles_e' and hasattr(self, 'angles_alongship_e') and
                hasattr(self, 'angles_athwartship_e')):
                    data_refs.append(getattr(self, 'angles_alongship_e'))
                    data_refs.append(getattr(self, 'angles_athwartship_e'))
            else:
                raise AttributeError("Unable to convert raw sample data. The attribute name " +
                        property_name + " does not exist.")

        # Populate the calibration parameters required for this method.
        # First, create a dict with key names that match the attributes names
        # of the calibration parameters we require for this method.
        cal_parms = {'sample_interval':None,
                     'sound_speed':None,
                     'sample_offset':None,
                     'transducer_mounting':None,
                     'drop_keel_offset': None,
                     'transducer_offset_z': None,
                     'water_level_draft': None}

        # Next, iterate through the dictionary calling the method to extract
        # the values for each parameter.
        for key in cal_parms:
            cal_parms[key] = calibration.get_parameter(self, key,
                    return_indices)

        # Check if we have multiple sample offset values and get the minimum.
        unique_sample_offsets = np.unique(
            cal_parms['sample_offset'][~np.isnan(cal_parms['sample_offset'])])
        min_sample_offset = min(unique_sample_offsets)

        # Check if we need to resample our sample data.
        unique_sample_interval = np.unique(
            cal_parms['sample_interval'][~np.isnan(cal_parms['sample_interval'])])

        # Iterate through our data types, processing each one. When requesting power,
        # there is only one reference. When requestig angle data, there will be two.
        # This is not an optimal solution, but a reasonable hack to handle complex data.
        for idx, data in enumerate(data_refs):
            if unique_sample_interval.shape[0] > 1:
                # There are at least 2 different sample intervals in the data.  We
                # must resample the data. We'll deal with adjusting sample offsets
                # here too.
                (output, sample_interval) = self._vertical_resample(data[return_indices],
                        cal_parms['sample_interval'], unique_sample_interval, resample_interval,
                        cal_parms['sample_offset'], min_sample_offset,
                        is_power = property_name == 'power')
            else:
                # We don't have to resample, but check if we need to shift any
                # samples based on their sample offsets.
                if unique_sample_offsets.shape[0] > 1:
                    # We have multiple sample offsets so we need to shift some of
                    # the samples.
                    output = self._vertical_shift(data[return_indices], cal_parms['sample_offset'],
                            unique_sample_offsets, min_sample_offset)
                else:
                    # The data all have the same sample intervals and sample
                    # offsets.  Simply copy the data as is.
                    output = data[return_indices].copy()

                # Get the sample interval value to use for range conversion below.
                sample_interval = unique_sample_interval[0]

            # Check if we have a fixed sound speed.
            unique_sound_velocity = np.unique(cal_parms['sound_speed'][~np.isnan(cal_parms['sound_speed'])])
            if unique_sound_velocity.shape[0] > 1:
                # There are at least 2 different sound speeds in the data or
                # provided calibration data.  Interpolate all data to the most
                # common range (which is the most common sound speed).
                sound_velocity = None
                n = 0
                for speed in unique_sound_velocity:
                    # Determine the sound speed with the most pings.
                    if np.count_nonzero(cal_parms['sound_speed'] == speed) > n:
                       sound_velocity = speed

                # Calculate the target range.
                range = get_range_vector(output.shape[1], sample_interval,
                        sound_velocity, min_sample_offset)

                # Now interpolate the samples for pings with different sound speeds
                for speed in unique_sound_velocity:
                    # Only interpolate the "other" speeds
                    if speed != sound_velocity:

                        # Get an array of indexes in the output array to interpolate.
                        pings_to_interp = np.where(cal_parms['sound_speed'] == speed)[0]

                        # Compute this data's range
                        resample_range = get_range_vector(output.shape[1], sample_interval,
                            cal_parms['sound_speed'][pings_to_interp[0]], min_sample_offset)

                        # Interpolate samples to target range
                        interp_f = interp1d(resample_range, output[pings_to_interp,:], axis=1,
                                bounds_error=False, fill_value=np.nan, assume_sorted=True)
                        output[pings_to_interp,:] = interp_f(resample_range)

            else:
                # We have a fixed sound speed and only need to calculate a single range vector.
                sound_velocity = unique_sound_velocity[0]
                range = get_range_vector(output.shape[1], sample_interval,
                        sound_velocity, min_sample_offset)

            #  Create the processed data object we will return
            p_data = processed_data(self.channel_id, f, None)

            # Populate it with time.
            p_data.ping_time = self.ping_time[return_indices].copy()

            # Assign the results to the "data" processed_data object.
            p_data.add_data_attribute('data', output)

            # Calculate the sample thickness.
            sample_thickness = sample_interval * sound_velocity / 2.0

            # Now assign range, sound_velocity, sample thickness and offset to
            # the processed_data object.
            p_data.add_data_attribute('range', range)
            p_data.sound_speed = sound_velocity
            p_data.sample_thickness = sample_thickness
            p_data.sample_offset = min_sample_offset
            p_data.sample_interval = sample_interval

            # Add the transducer offset attribute.
            xdcr_draft = self._get_transducer_offset(cal_parms)
            p_data.add_data_attribute('transducer_offset', xdcr_draft)

            # Add the heave attribute is available.
            _, heave_data = self.motion_data.interpolate(p_data, 'heave')
            if  heave_data['heave'] is not None:
                p_data.add_data_attribute('heave', heave_data['heave'])

            data_refs[idx] = p_data

        # Return the processed_data object containing the requested data.
        data_refs.append(return_indices)
        return data_refs


    def _get_transducer_offset(self, cal_parms):
        """Computes the transducer offset given the z offset, hull draft, drop keel offset
        and or tow body depth. Transducer offset is one component of transducer draft.
        Transducer draft = transducer offset + heave

        This is an internal method.
        """

        # Start with no draft
        xdcr_draft = np.full((cal_parms['transducer_offset_z'].shape[0]),
                0.0, dtype=np.float32)

        # Apply the transducer z offset
        has_z_param = np.isfinite(cal_parms['transducer_offset_z'])
        xdcr_draft[has_z_param] += cal_parms['transducer_offset_z'][has_z_param]

        # Add the draft from the mounting - first we need to check if the data
        # has the transducer_mounting parameter
        if (cal_parms['transducer_mounting'] is not None and
                cal_parms['transducer_mounting'].dtype == 'O'):

            # It does, find where the mounting is drop keel
            has_d_param = cal_parms['transducer_mounting'] == 'DropKeel'

            # Add the drop keel offsets. This is not done in an obvious way.
            # The z offset written in the configuration header is the total
            # transducer offset at the time the EK80 program is started which
            # includes the individual transducer z ofsets + the initial drop
            # keel offset(and I would assume water level draft or tow body depth)
            # so the drop keel offset we add here is relative to that starting
            # point.
            has_param = np.logical_and(has_d_param, has_z_param)
            if np.any(has_param):
                xdcr_draft[has_param] += cal_parms['transducer_offset_z'][has_param] + \
                        (cal_parms['drop_keel_offset'][has_param] -
                        cal_parms['transducer_offset_z'][has_param])

            # Then find where the mounting is hull mounted and add the water level
            # draft. I would assume the same logic applies here...
            has_d_param = cal_parms['transducer_mounting'] == 'HullMounted'
            has_param = np.logical_and(has_d_param, has_z_param)
            if np.any(has_param):
                xdcr_draft[has_param] += cal_parms['transducer_offset_z'][has_param] + \
                        (cal_parms['water_level_draft'][has_param] -
                        cal_parms['transducer_offset_z'][has_param])


            # I believe the tow body transducer mounting and tow body depth were
            # introduced in EK80 v2.0.0? I would assume the same logic applies
            # but I'm not implementing that at this time.

        return xdcr_draft


    def _convert_power(self, power_data, calibration, convert_to, linear,
            return_indices, tvg_correction):
        """Converts power to Sv/sv/Sp/sp

        Args:
            power_data (processed_data): A processed_data object with the raw power
                data read from the file.
            calibration (calibration object): The data calibration object where
                calibration data will be retrieved.
            convert_to (str):  A string that specifies what to convert the
                power to.  Possible values are: Sv, sv, Sp, or sp.
            linear (bool):  Set to True to return linear values.
            return_indices (array): A numpy array of indices to return.
            tvg_correction (bool): Set to True to apply a correction to the
                range of 2 * sample thickness.

        Returns:
            An array with the converted data.
        """

        # Populate the calibration parameters required for this method.
        # First, create a dictionary with key names that match the attribute
        # names of the calibration parameters we require for this method.
        cal_parms = {'gain':None,
                     'transmit_power':None,
                     'equivalent_beam_angle':None,
                     'pulse_duration':None,
                     'absorption_coefficient':None,
                     'sa_correction':None,
                     'pulse_form':None}

        # Next, iterate through the dictionary, calling the method to extract
        # the values for each parameter.
        for key in cal_parms:
            cal_parms[key] = calibration.get_parameter(self, key,
                    return_indices)

        # Get sound_velocity from the power data since get_power might have
        # manipulated this value. Remember that we're operating on a
        # processed_data object so all pings share the same sound speed.
        cal_parms['sound_speed'] = np.empty((return_indices.shape[0]),
                dtype=np.float32)
        cal_parms['sound_speed'].fill(power_data.sound_speed)

        # For EK60 hardware use pulse duration when computing gains
        # but for EK80 hardware use effectve pulse duration.
        if self.transceiver_type == 'GPT':
            effective_pulse_duration = cal_parms['pulse_duration']
        else:
            effective_pulse_duration = calibration.get_parameter(self,
                'effective_pulse_duration', return_indices)

        # Calculate the system gains.
        wavelength = cal_parms['sound_speed'] / power_data.frequency
        if convert_to in ['sv','Sv']:
            gains = 10 * np.log10((cal_parms['transmit_power'] * (10**(cal_parms['gain'] / 10.0))**2 *
                wavelength**2 * cal_parms['sound_speed'] * effective_pulse_duration *
                10**(cal_parms['equivalent_beam_angle']/10.0)) / (32 * np.pi**2))
        else:
            gains = 10 * np.log10((cal_parms['transmit_power'] * (10**(
                cal_parms['gain']/10.0))**2 * wavelength**2) / (16 * np.pi**2))

        # Get the range for TVG calculation.  The method used depends on the
        # hardware used to collect the data.
        c_range = np.zeros(power_data.shape[0:2], dtype=power_data.sample_dtype)
        c_range += power_data.range

        if tvg_correction:
            if self.transceiver_type == 'GPT':
                # For the Ex60 hardware, the corrected range is computed as:
                #   c_range = range - (2 * sample_thickness)
                c_range -= (2.0 * power_data.sample_thickness)
            else:
                # For the Ex80 WBT style hardware corrected range is computed as:
                #    c_range = range - (sound speed * transmitted pulse length / 4)
                c_range -= (power_data.sound_speed * cal_parms['pulse_duration'] / 4.0)[:,np.newaxis]

            #  zero out negative ranges
            c_range[c_range < 0] = 0

        # Calculate time varied gain. Do not apply at ranges < 1 m.
        tvg = c_range.copy()
        tvg[tvg < 1] = 1
        if convert_to in ['sv','Sv']:
            tvg = 20.0 * np.log10(tvg)
        else:
            tvg = 40.0 * np.log10(tvg)

        # Calculate absorption - our starting point.
        data = (2.0 * cal_parms['absorption_coefficient'])[:,np.newaxis] * c_range

        # Add in power and TVG.
        data += power_data.data + tvg

        # Subtract the applied gains.
        data -= gains[:, np.newaxis]

        # Apply sa correction for Sv/sv. As of 3/17/21 Echoview doesn't apply
        # Sa Correction to FM Sv data. We're going to do the same thing here
        # until advised otherwise.
        if convert_to in ['sv','Sv']:
            # Only apply to CW data
            not_fm = cal_parms['pulse_form'] == 0
            data[not_fm,:] -= (2.0 * cal_parms['sa_correction'])[not_fm, np.newaxis]

        # Check if we're returning linear or log values.
        if linear:
            # Convert to linear units.
            data[:] = 10 ** (data / 10.0)

        # Return the result.
        return data


    def _create_arrays(self, n_pings, n_samples, initialize=False, create_power=False,
            create_angles=False, create_complex=True, n_complex=4):
        """Initializes raw_data data arrays.

        This is an internal method. Note that all arrays must be numpy arrays.

        Args:
            n_pings (int): Number of pings.
            n_samples (int): Number of samples.
            initialize (bool): Set to True to initialize arrays.
            create_power (bool): Set to True to create the power attribute.
            create_angles (bool): Set to True to create the angles_alongship_e
                                  angles_athwartship_e attributes.
            create_complex (bool): Set to True to create the complex attribute.
            n_complex (int): Number of complex values per sample
        """

        # First, create uninitialized arrays.
        self.ping_time = np.empty((n_pings), dtype='datetime64[ms]')

        #  create the arrays that contain references to the async data objects
        self.configuration = np.empty((n_pings), dtype='object')
        self.environment = np.empty((n_pings), dtype='object')
        self.filters = np.empty((n_pings), dtype='object')

        #  the rest of the arrays store syncronous ping data
        self.sample_offset = np.empty((n_pings), np.int32)
        self.channel_mode = np.empty((n_pings), np.int32)
        self.pulse_form = np.empty((n_pings), np.int32)
        if self.data_type == 'complex-FM':
            self.frequency_start = np.empty((n_pings), np.float32)
            self.frequency_end = np.empty((n_pings), np.float32)
        else:
            self.frequency = np.empty((n_pings), np.float32)
        self.pulse_duration = np.empty((n_pings), np.float32)
        self.sample_interval = np.empty((n_pings), np.float32)
        self.slope = np.empty((n_pings), np.float32)
        self.transmit_power = np.empty((n_pings), np.float32)
        self.sample_count = np.empty((n_pings), np.int32)

        #  and the 2d sample data arrays
        if create_power and self.store_power:
            self.power = np.empty((n_pings, n_samples),
                dtype=self.sample_dtype, order='C')
            self.n_samples = n_samples
            self._data_attributes.append('power')
        if create_angles and self.store_angles:
            self.angles_alongship_e = np.empty((n_pings, n_samples),
                dtype=self.sample_dtype, order='C')
            self._data_attributes.append('angles_alongship_e')
            self.angles_athwartship_e = np.empty((n_pings, n_samples),
                dtype=self.sample_dtype, order='C')
            self._data_attributes.append('angles_athwartship_e')
            self.n_samples = n_samples
        if create_complex and self.store_complex:
            self.complex = np.empty((n_pings, n_samples,n_complex),
                dtype=self.sample_dtype, order='C')
            self._data_attributes.append('complex')
            self.n_complex = n_complex
            self.complex_dtype = np.empty((n_pings), dtype='object')
            self.n_samples = n_samples

        #  update our shape attribute
        self._shape()

        # Check if we should initialize them.
        if initialize:
            self.ping_time.fill(np.datetime64('NaT'))
            self.sample_offset.fill(np.nan)
            self.channel_mode.fill(0)
            self.pulse_form.fill(0)
            self.frequency.fill(np.nan)
            self.pulse_duration.fill(np.nan)
            self.sample_interval.fill(np.nan)
            self.slope.fill(np.nan)
            self.transmit_power.fill(np.nan)
            self.sample_count.fill(0)

            if create_angles and self.store_power:
                self.power.fill(np.nan)
            if create_angles and self.store_angles:
                self.angles_alongship_e.fill(np.nan)
                self.angles_athwartship_e.fill(np.nan)
            if create_complex and self.store_complex:
                self.complex.fill(np.nan)


    def match_pings(self, other_data, match_to='cs'):
        """Matches the ping times in this object to the ping times in the EK80.raw_data
        object provided. It does this by matching times, inserting and/or deleting
        pings as needed. It does not interpolate. Ping times in the other object
        that aren't in this object are inserted. Ping times in this object that
        aren't in the other object are deleted. If the time axes do not intersect
        at all, all of the data in this object will be deleted and replaced with
        empty pings for the ping times in the other object.


        Args:
            other_data (ping_data): A ping_data type object that this object
            will be matched to.

            match_to (str): Set to a string defining the precision of the match.

                cs : Match to a 100th of a second
                ds : Match to a 10th of a second
                s  : Match to the second

        Returns:
            A dictionary with the keys 'inserted' and 'removed' containing the
            indices of the pings inserted and removed.
        """
        return super(processed_data, self).match_pings(other_data, match_to='cs')


    def __str__(self):
        """
        Reimplemented string method that provides some basic info about the
        raw_data object.
        """

        # Print the class and address.
        msg = str(self.__class__) + " at " + str(hex(id(self))) + "\n"

        # check if first ping is FM or CW
        is_fm = self.pulse_form[0] > 0

        # Print some more info about the EK80.raw_data instance.
        n_pings = len(self.ping_time)
        if n_pings > 0:
            msg = msg + "                   channel: " + self.channel_id + "\n"
            if is_fm:
                msg = msg + "frequency start (first ping): " + str(
                    self.frequency_start[0]) + "\n"
                msg = msg + "  frequency end (first ping): " + str(
                    self.frequency_end[0]) + "\n"
            else:
                msg = msg + "    frequency (first ping): " + str(
                    self.frequency[0]) + "\n"
            msg = msg + "   pulse length (first ping): " + str(
                self.pulse_duration[0]) + "\n"
            msg = msg + "             data start time: " + str(
                self.ping_time[0]) + "\n"
            msg = msg + "               data end time: " + str(
                self.ping_time[n_pings-1]) + "\n"
            msg = msg + "             number of pings: " + str(n_pings) + "\n"
            if hasattr(self, 'power'):
                n_pings,n_samples = self.power.shape
                msg = msg + ("                  data type: CW reduced\n")
                msg = msg + ("     power array dimensions: (" + str(n_pings) +
                             "," + str(n_samples) + ")\n")
            if hasattr(self, 'angles_alongship_e'):
                n_pings,n_samples = self.angles_alongship_e.shape
                msg = msg + ("     angle array dimensions: (" + str(n_pings) +
                             "," + str(n_samples) + ")\n")
            if hasattr(self, 'complex'):
                n_pings,n_samples,n_sectors = self.complex.shape
                if is_fm:
                    msg = msg + ("                  data type: FM\n")
                else:
                    msg = msg + ("                  data type: CW complex\n")
                msg = msg + ("   complex array dimensions: (" + str(n_pings) +
                             "," + str(n_samples) + "," + str(n_sectors) + ")\n")
        else:
            msg = msg + "  raw_data object contains no data\n"

        return msg


class ek80_calibration(calibration):
    """
    The calibration class contains parameters required for transforming power,
    electrical angle, and complex data to Sv/sv TS/SigmaBS and physical angles.

    When converting raw data to power, Sv/sv, Sp/sp, or to physical angles
    you have the option of passing a calibration object containing the data
    you want used during these conversions. To use this object you create an
    instance and populate the attributes with your calibration data.

    You can provide the data in 2 forms:
        As a scalar - the single value will be used for all pings.
        As a vector - a vector of values as long as the number of pings
            in the raw_data object where the first value will be used
            with the first ping, the second with the second, and so on.

    If you set any attribute to None, that attribute's values will be obtained
    from the raw_data object which contains the value at the time of recording.
    If you do not pass a calibration object to the conversion methods
    *all* of the cal parameter values will be extracted from the raw_data
    object.
    """

    def __init__(self, absorption_method='F&G'):
        '''Create an instance of an ek80_calibration object.

        '''

        #  call the parent init
        super(ek80_calibration, self).__init__(absorption_method=absorption_method)

        # Set up the attributes that are specific to the EK80 system. In general
        # this wttribute names will match the parameter names found within the
        # EK80 formatted .raw file except that

        # Absorption_method stores the string identifying the method used to
        # compute seawater absorption. Unlike ER60, EK80 doesn't compute this
        # and instead provides the data to do it.
        self.absorption_method = absorption_method

        # Create a dict containing the hardware sampling frequencies of various
        # Simrad hardware. In EK80 versions prior to 1.12.2 the rx_sampling_frequency
        # configuration property didn't exist. When these files are read, they use
        # the value here based in the transceiver_type configuration value.
        self.default_sampling_frequency = {'GPT':500000,
                                           'SBT':50000,
                                           'WBAT':1500000,
                                           'WBT TUBE':1500000,
                                           'WBT MINI':1500000,
                                           'WBT':1500000,
                                           'WBT HP':187500,
                                           'WBT LF':93750}

        #  these attributes are properties of the raw_data class
        self._raw_attributes = ['frequency','frequency_start','frequency_end', 'transmit_power',
                'pulse_duration' ,'sample_offset', 'channel_mode', 'pulse_form',
                'sample_interval' ,'slope' ,'sample_count','filters']
        self._init_attributes(self._raw_attributes)

        #  these attributes are found in the configuration datagram
        self._config_attributes = ['pulse_duration_fm', 'gain' ,'sa_correction', 'equivalent_beam_angle',
                'angle_sensitivity_alongship', 'angle_sensitivity_athwartship', 'angle_offset_alongship',
                'angle_offset_athwartship', 'beam_width_alongship', 'beam_width_athwartship',
                'directivity_drop_at_2x_beam_width', 'transducer_offset_x', 'transducer_offset_y',
                'transducer_offset_z', 'transducer_alpha_x', 'transducer_alpha_y', 'transducer_alpha_z',
                'transducer_name', 'hw_channel_configuration', 'rx_sample_frequency', 'time_bias',
                'transducer_mounting','transceiver_type','impedance', 'transducer_frequency',
                'transducer_frequency_minimum','transducer_frequency_maximum','transducer_beam_type']
        self._init_attributes(self._config_attributes)

        # These attributes are found in the environment datagrams
        self._environment_attributes = ['depth', 'acidity', 'salinity', 'sound_speed', 'temperature',
                'latitude', 'transducer_sound_speed', 'sound_velocity_profile', 'drop_keel_offset',
                'water_level_draft']
        self._init_attributes(self._environment_attributes)


        # Add "special" attributes - These are attributes that require specific handling

        # For the EK80, absorption_coefficient is computed so we compute it when requested
        self.absorption_coefficient = None

        # effective_pulse_length is also computed
        self.effective_pulse_duration = None


    def from_raw_data(self, raw_data, return_indices=None):
        """Populates the calibration object.

        This method uses the values extracted from a raw_data object to
        populate the calibration object.

        Args:
            raw_data (raw_data): The object where parameters will be extracted
                from and used to populate the calibration object.
            return_indices (array): A numpy array of indices to return.
        """

        #  call the parent method
        super(ek80_calibration, self).from_raw_data(raw_data,
                return_indices=return_indices)

        # Handle our special attributes

        # EK80 doesn't store absorption directly, but instead it
        # stores the bits needed to compute it.
        self.absorption_coefficient = self._compute_absorption(raw_data,
            return_indices, self.absorption_method)

        self.effective_pulse_duration = self.get_attribute_from_raw(raw_data,
                param_name='effective_pulse_duration', return_indices=return_indices)


    def get_attribute_from_raw(self, raw_data, param_name, return_indices=None):
        """get_attribute_from_raw gets an individual attribute using the data
        within the provided raw_data object.
        """

        #  first call the parent method to get the "standard" attributes
        param_data, return_indices = super(ek80_calibration, self).get_attribute_from_raw(
                raw_data, param_name, return_indices=return_indices)

        # Now handle attributes that need some special handling These are mostly
        # parameters that must be looked up from a lookup table.

        # Files written with EK80 software versions < 1.12.0 lacked xcvr impedance
        # info in the file header. The EK80 software used 1000 ohms so we're going
        # to assume that's what we should use here.
        if param_name == 'impedance':
            if param_data is None or np.isnan(param_data):
                param_data = raw_data.ZTRANSCEIVER

        # rx_sample_frequency is a special case because it is a parameter
        # that was recently added to the EK80 raw file configuration datagram
        # and there will be files that do not contain it. In these cases we
        # look up a default value based on the transceiver hardware.
        if param_name == 'rx_sample_frequency':
            #  create the return array
            param_data = np.empty((return_indices.shape[0]), dtype=np.float32)

            #  loop thru the indices extracting the values
            ret_idx = 0
            for idx in return_indices:
                #  check if this config object has the rx_sample_frequency key
                conf_keys = raw_data.configuration[idx].keys()
                if 'rx_sample_frequency' in conf_keys:
                    #  yes - get the value
                    param_data[ret_idx] = raw_data.configuration[idx]['rx_sample_frequency']
                else:
                    # no - this data is older. Get the value from the
                    # default_sampling_frequency dict using the transceiver type as key
                    t_type = raw_data.configuration[idx]['transceiver_type'].upper()
                    param_data[ret_idx] = self.default_sampling_frequency[t_type]
                ret_idx += 1

        elif param_name == 'sa_correction':
            # sa correction is extracted from a list that is indexed by pulse duration
            new_data = np.empty((return_indices.shape[0]), dtype=np.float32)

            for idx, config_obj in enumerate(raw_data.configuration[return_indices]):
                if raw_data.pulse_form[idx] > 0:
                    # For now we assume that just like with gain, we use the pulse_duration table even
                    # though there is also a pulse_duration_fm table.
                    match_idx = np.where(config_obj['pulse_duration'] == raw_data.pulse_duration[idx])[0]
                    if not match_idx:
                        # OK, I think that at some point at or around v2.1.? the EK80 software
                        # changed what it stores in the pulse_duration_fm table? The pulse duration
                        # stored for FM data now is found in pulse_duration_fm
                        match_idx = np.where(config_obj['pulse_duration_fm'] == raw_data.pulse_duration[idx])[0]
                    new_data[idx] = param_data[match_idx][0]

                else:
                    if param_data.ndim == 1:
                        new_data[idx] = param_data[config_obj['pulse_duration'] == raw_data.pulse_duration[idx]][0]
                    else:
                       new_data[idx] = param_data[idx, config_obj['pulse_duration'] == raw_data.pulse_duration[idx]][0]

            param_data = new_data

        elif param_name == 'gain':
            # Gain needs to be handled differently depending on if the data are FM or CW.
            new_data = np.empty((return_indices.shape[0]), dtype=np.float32)

            # Get the frequencies - this returns the center freq for FM
            frequency = raw_data.get_frequency()

            # Work thru the pings, extracting the params
            for idx, config_obj in enumerate(raw_data.configuration[return_indices]):
                # check if this is an fm ping
                if raw_data.pulse_form[idx] > 0:
                    # We try to obtain FM gain first from the wideband transducer parameters. The
                    # transducer_params_wideband key will exist if the data were collected from a
                    # system with a broadband cal applied.
                    if 'transducer_params_wideband' in config_obj:
                        # It looks like there will not always be a match in the transducer_params_wideband
                        # table for the center freq so we have to find the closest. Wish I would have
                        # known since the current implementation is not optimal for this.
                        xdcr_parms_wb_freqs = np.array(list(config_obj['transducer_params_wideband'].keys()))

                        #  I'm not sure if using the closest value or an interpolated value is better.
                        if False:
                            #  use the closest value we find in our table
                            table_freq = xdcr_parms_wb_freqs[np.abs(xdcr_parms_wb_freqs - frequency[idx]).argmin()]
                            new_data[idx] = config_obj['transducer_params_wideband'][table_freq]['gain']
                        else:
                            #  use an interpreted gain value
                            gains = []
                            for f in xdcr_parms_wb_freqs:
                                gains.append(config_obj['transducer_params_wideband'][f]['gain'])
                            new_data[idx] = np.interp(frequency[idx], xdcr_parms_wb_freqs, gains)
                    else:
                        # per Echoview bug fix notification, it seems that the 'pulse_duration' table is used
                        # for FM data without BB cal even though the 'pulse_duration_fm' table exists.
                        # https://www.echoview.com/products-services/news/echoview-bug-fix-correction-to-transducer-gain-calculations-in-simrad-ek80-wideband-data
                        match_idx = np.where(config_obj['pulse_duration'] == raw_data.pulse_duration[idx])[0]
                        gain = param_data[match_idx][0]

                        new_data[idx] = gain + (20 * np.log10(frequency[idx] / config_obj['transducer_frequency']))
                else:
                    # CW gain is obtained from the gain table that is indexed by pulse duration
                    if param_data.ndim == 1:
                        new_data[idx] = param_data[config_obj['pulse_duration'] == raw_data.pulse_duration[idx]][0]
                    else:
                        new_data[idx] = param_data[idx, config_obj['pulse_duration'] == raw_data.pulse_duration[idx]][0]
            param_data = new_data

        elif param_name in ['angle_offset_alongship', 'angle_offset_athwartship',
                            'beam_width_alongship', 'beam_width_athwartship']:
            # beamwidths and offsets need to be handled differently depending too.
            new_data = np.empty((return_indices.shape[0]), dtype=np.float32)

            # Get the frequency - this returns the center freq for FM
            frequency = raw_data.get_frequency()

            # Work thru the pings, extracting the params
            for idx, config_obj in enumerate(raw_data.configuration[return_indices]):
                # check if this is an fm ping with a broadband cal
                if raw_data.pulse_form[idx] > 0 and 'transducer_params_wideband' in config_obj:
                    # The structure of the transducer_params_wideband dict makes extracting
                    # params like this awkward.
                    xdcr_parms_wb_freqs = np.array(list(config_obj['transducer_params_wideband'].keys()))

                    #  I'm not sure if using the closest value or an interpolated value is better.
                    if False:
                        #  use the closest value we find in our table
                        table_freq = xdcr_parms_wb_freqs[np.abs(xdcr_parms_wb_freqs - frequency[idx]).argmin()]
                        new_data[idx] = config_obj['transducer_params_wideband'][table_freq][param_name]
                    else:
                        #  use an interpreted gain value
                        vals = []
                        for f in xdcr_parms_wb_freqs:
                            vals.append(config_obj['transducer_params_wideband'][f][param_name])
                        new_data[idx] = np.interp(frequency[idx], xdcr_parms_wb_freqs, vals)

                    param_data = new_data

        elif param_name == 'equivalent_beam_angle':
            # equivalent_beam_angle is computed differently for FM and CW applications
            new_data = np.empty((return_indices.shape[0]), dtype=np.float32)

            # Get the frequencies - this returns the center freq for FM
            frequency = raw_data.get_frequency()

            # Work thru the pings, extracting the params
            for idx, config_obj in enumerate(raw_data.configuration[return_indices]):
                # check if this is an fm ping
                if raw_data.pulse_form[idx] > 0:
                    new_data[idx] = param_data + (20 * np.log10(config_obj['transducer_frequency'] / frequency[idx]))
                else:
                    new_data[idx] = param_data

            param_data = new_data

        elif param_name == 'effective_pulse_duration':
            # EK80 hardware uses the effective_pulse_duration
            if self.transceiver_type != 'GPT':
                # For now we're always converting to pulse compressed Sv/Sp so we return the
                # effective pulse duration of the pulse compressed Tx signal
                return_pc = True
                param_data = simrad_signal_proc.compute_effective_pulse_duration(raw_data, self,
                        return_pc=return_pc, return_indices=return_indices)
            else:
                # EK60 hardware power conversion does not require this parameter
                param_data = None

        return param_data


    def __str__(self):
        """Re-implements string method that provides some basic info about
        the ek80_calibration object

        Returns:
            A string with information about the ek80_calibration instance.
        """

        #  print the class and address
        msg = str(self.__class__) + " at " + str(hex(id(self))) + "\n"

        # Create a list of attributes to print out - I'm just adding them
        # all right now. Can set specific attributes if this is too much.
        attr_to_display = []
        attr_to_display.extend(self._raw_attributes)
        attr_to_display.extend(self._config_attributes)
        attr_to_display.extend(self._environment_attributes)
        attr_to_display.remove('filters')

        # And assemble the string
        for param_name in attr_to_display:
            n_spaces = 34 - len(param_name)
            msg += n_spaces * ' ' + param_name
            # Extract data from raw_data attribues
            if hasattr(self, param_name):
                attr = getattr(self, param_name)
                if isinstance(attr, np.ndarray):
                    msg += ' :: array ' + str(attr.size) + ' :: First value: ' + str(attr[0]) + '\n'
                else:
                    if attr is not None:
                            msg += ' :: scalar :: Value: ' + str(attr) + '\n'
                    else:
                        msg += ' :: No value set\n'

        return msg


def read_config(raw_file):
    '''read_config reads the configuration header from a Simrad EK80 .raw file
    and returns the parsed configuration datagram in a dictionary. This
    method can be used to quickly read the configuration data when you don't
    need to read any of the raw data.

    Args:
    raw_file (string): The full path to the raw file you want to read.

    '''

    #  normalize the file path
    raw_file = os.path.normpath(raw_file)

    # open the raw file
    fid = RawSimradFile(raw_file, 'r')

    #  read the configuration datagram
    config_datagram = fid.read(1)

    #  pack the datagram timestamp into the config dict
    config_datagram['configuration']['timestamp'] = \
            np.datetime64(config_datagram['timestamp'], '[ms]')

    #  and return the config data dict
    return config_datagram['configuration']
